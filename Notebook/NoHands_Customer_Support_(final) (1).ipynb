{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"NoHands-Customer-Support\""
      ],
      "metadata": {
        "id": "JH5iBofA_4zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3iZ0hCkqfCw"
      },
      "source": [
        "# AI-Powered Customer Support System for Digital Music Store\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project builds an intelligent, multi-agent customer support system for a digital music store using LangGraph and LangSmith. The system automatically handles customer inquiries about music purchases, invoices, and catalog browsing while maintaining conversation history and user preferences.\n",
        "\n",
        "### Real-World Use Case\n",
        "\n",
        "Imagine you run a digital music platform like iTunes or Spotify. Your customers frequently ask questions like:\n",
        "- \"Can you recommend some jazz albums?\"\n",
        "- \"What did I purchase last month?\"\n",
        "- \"Show me my recent invoices\"\n",
        "- \"I'm looking for classical music by Mozart\"\n",
        "\n",
        "This system handles all these queries automatically by:\n",
        "1. **Verifying customer identity** (human-in-the-loop security)\n",
        "2. **Loading user preferences** from memory (personalization)\n",
        "3. **Routing queries intelligently** to specialized agents:\n",
        "   - Music Catalog Agent: Searches and recommends music\n",
        "   - Invoice Agent: Retrieves purchase history and billing info\n",
        "4. **Remembering conversation context** for better future interactions\n",
        "\n",
        "### Architecture\n",
        "\n",
        "```\n",
        "Customer Query\n",
        "     ↓\n",
        "[Identity Verification] ← Human approval required\n",
        "     ↓\n",
        "[Load User Preferences] ← Long-term memory\n",
        "     ↓\n",
        "[Supervisor Agent] ← Routes to appropriate specialist\n",
        "     ↓\n",
        "  ┌──────────────┐\n",
        "  │              │\n",
        "[Music Agent] [Invoice Agent]\n",
        "  │              │\n",
        "  └──────────────┘\n",
        "     ↓\n",
        "[Save Preferences] → Long-term memory\n",
        "     ↓\n",
        "Customer Response\n",
        "```\n",
        "\n",
        "### Technologies Used\n",
        "- **LangGraph**: Multi-agent orchestration\n",
        "- **LangChain**: LLM integration and tooling\n",
        "- **OpenAI GPT-4**: Language model\n",
        "- **SQLite (Chinook DB)**: Sample music store database\n",
        "- **LangSmith**: Monitoring and debugging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Installation\n",
        "\n",
        "### What this section does:\n",
        "Installs all required Python packages and imports them in one centralized location.."
      ],
      "metadata": {
        "id": "AUMExMcX8ejR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain langchain-openai langchain-community langgraph langsmith langchain_chroma langgraph_supervisor langgraph_swarm openevals\n",
        "!pip install -q python-dotenv\n"
      ],
      "metadata": {
        "id": "BZbTsnWa8XeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI , OpenAIEmbeddings # Import the OpenAI chat model\n",
        "\n",
        "\n",
        "import sqlite3 # Standard Python library for SQLite database interaction\n",
        "import requests # Library for making HTTP requests (to download the SQL script)\n",
        "from langchain_community.utilities.sql_database import SQLDatabase # LangChain utility to interact with SQL databases\n",
        "from sqlalchemy import create_engine # SQLAlchemy function to create a database engine\n",
        "from sqlalchemy.pool import StaticPool # SQLAlchemy connection pool class for in-memory databases\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver # For short-term memory (thread-level state persistence)\n",
        "from langgraph.store.memory import InMemoryStore # For long-term memory (storing user preferences)\n",
        "\n",
        "from typing_extensions import TypedDict # For defining dictionaries with type hints\n",
        "from typing import Annotated, List # For type hinting lists and adding annotations\n",
        "from langgraph.graph.message import AnyMessage, add_messages # For managing messages in the graph state\n",
        "from langgraph.managed.is_last_step import RemainingSteps # For tracking recursion limits\n",
        "from langgraph_supervisor import create_supervisor # Import the pre-built supervisor creator\n",
        "\n",
        "from langchain_core.tools import tool # Decorator to define a function as a LangChain tool\n",
        "import ast # Module to safely evaluate strings containing Python literal structures\n",
        "\n",
        "\n",
        "from langgraph.prebuilt import ToolNode # Pre-built node for executing tools\n",
        "\n",
        "from langchain_core.messages import ToolMessage, SystemMessage, HumanMessage # Message types for conversation history\n",
        "from langchain_core.runnables import RunnableConfig # For configuration parameters passed to runnables\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END # Core LangGraph classes and special node names\n",
        "from langchain_core.tools import tool # Import the tool decorator again\n",
        "\n",
        "from langsmith import traceable"
      ],
      "metadata": {
        "id": "jhYNPAavtb2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration and Environment Setup\n",
        "\n",
        "### What this section does:\n",
        "Loads API keys and configures the connection to our sample music store database (Chinook). This database contains realistic data about customers, purchases, and music catalog."
      ],
      "metadata": {
        "id": "tKfUjkRm9Asd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0A_3ToSqfCx"
      },
      "outputs": [],
      "source": [
        "# Initialize the ChatOpenAI model. We're using a specific model from Llama 3.3 series.\n",
        "# This `model` object will be used throughout the notebook for all LLM interactions.\n",
        "\n",
        "openai_api_key = userdata.get('openai_api_key')\n",
        "openai_endpoint = userdata.get('openai_endpoint')\n",
        "\n",
        "llm = ChatOpenAI(api_key = openai_api_key ,\n",
        "                 base_url = openai_endpoint,\n",
        "                 model = 'gpt-4o-mini' ,\n",
        "                 temperature = 0\n",
        "                 )\n",
        "\n",
        "Embedding_model = OpenAIEmbeddings(api_key = openai_api_key ,\n",
        "                 base_url = openai_endpoint,\n",
        "                 model = 'text-embedding-3-small' ,\n",
        "                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rTjD8PTqfCx"
      },
      "source": [
        "#### Loading Sample Customer Data (Chinook Database)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtmHBvyYqfCx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_engine_for_chinook_db():\n",
        "    \"\"\"Pull sql file, populate in-memory database, and create engine.\"\"\"\n",
        "    # URL to the raw SQL script for the Chinook database\n",
        "    url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\"\n",
        "\n",
        "    # Fetch the SQL script content from the URL\n",
        "    response = requests.get(url)\n",
        "    sql_script = response.text\n",
        "\n",
        "    # Create an in-memory SQLite database connection.\n",
        "    # `check_same_thread=False` is important for SQLAlchemy's StaticPool.\n",
        "    connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
        "\n",
        "    # Execute the SQL script to populate the in-memory database with Chinook data\n",
        "    connection.executescript(sql_script)\n",
        "\n",
        "    # Create a SQLAlchemy engine for the in-memory SQLite database.\n",
        "    # `creator=lambda: connection` tells SQLAlchemy how to get a new connection.\n",
        "    # `poolclass=StaticPool` is used for in-memory databases, ensuring the same connection is reused.\n",
        "    # `connect_args` are passed directly to the `sqlite3.connect` function.\n",
        "    return create_engine(\n",
        "        \"sqlite://\",\n",
        "        creator=lambda: connection,\n",
        "        poolclass=StaticPool,\n",
        "        connect_args={\"check_same_thread\": False},\n",
        "    )\n",
        "\n",
        "# Get the SQLAlchemy engine for our Chinook database\n",
        "engine = get_engine_for_chinook_db()\n",
        "\n",
        "# Create a LangChain SQLDatabase utility instance from the engine.\n",
        "# This utility will help our agents interact with the database via SQL queries.\n",
        "db = SQLDatabase(engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZtAJOriqfCy"
      },
      "source": [
        "#### Setting up Short-Term and Long-Term Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTxKAIKRqfCy"
      },
      "outputs": [],
      "source": [
        "# Initializing `InMemoryStore` for long-term memory.\n",
        "# This store will hold user-specific data like music preferences across sessions.\n",
        "in_memory_store = InMemoryStore()\n",
        "\n",
        "# Initializing `MemorySaver` for short-term (thread-level) memory.\n",
        "# This checkpointer saves the graph's state after each step, allowing for restarts or interruptions within a thread.\n",
        "checkpointer = MemorySaver()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zVbadkRqfCy"
      },
      "source": [
        "## Part 1: Building ReAct Sub-Agents\n",
        "\n",
        "\n",
        "### 1.1 Building a ReAct Agent from Scratch: The Music Catalog Sub-Agent\n",
        "\n",
        "#### State\n",
        "\n",
        "For our customer support agent, the state will track the following key elements:\n",
        "\n",
        "1.  `customer_id`: A string representing the ID of the customer interacting with the agent. This is crucial for personalized queries (e.g., checking invoice history).\n",
        "2.  `messages`: An annotated list of `AnyMessage` objects. This forms the conversation history, including user inputs, agent responses, and tool outputs. `add_messages` ensures that new messages are appended to the list, maintaining the conversational flow.\n",
        "3.  `loaded_memory`: A string that will hold any user preferences or relevant information loaded from the long-term memory store. This allows the agent to tailor responses based on past interactions.\n",
        "4.  `remaining_steps`: A `RemainingSteps` object. This is part of LangGraph's managed state and helps track the number of steps left before a recursion limit is hit, preventing infinite loops in cyclic graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9ZzsUBBqfCz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class State(TypedDict):\n",
        "    \"\"\"Represents the state of our LangGraph agent.\"\"\"\n",
        "    # customer_id: Stores the unique identifier for the current customer.\n",
        "    customer_id: str\n",
        "\n",
        "    # messages: A list of messages that form the conversation history.\n",
        "    # Annotated with `add_messages` to ensure new messages are appended rather than overwritten.\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "\n",
        "    # loaded_memory: Stores information loaded from the long-term memory store,\n",
        "    # typically user preferences or historical context.\n",
        "    loaded_memory: str\n",
        "\n",
        "    # remaining_steps: Used by LangGraph to track the number of allowed steps\n",
        "    # to prevent infinite loops in cyclic graphs.\n",
        "    remaining_steps: RemainingSteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPBCwlsKqfCz"
      },
      "source": [
        "#### Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9OCbOh4qfCz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "@tool\n",
        "def get_albums_by_artist(artist: str):\n",
        "    \"\"\"Get albums by an artist.\"\"\"\n",
        "    # Execute a SQL query to retrieve album titles and artist names\n",
        "    # from the Album and Artist tables, joining them and filtering by artist name.\n",
        "    # `db.run` is a utility from LangChain's SQLDatabase to execute queries.\n",
        "    # `include_columns=True` ensures column names are included in the result for better readability.\n",
        "    return db.run(\n",
        "        f\"\"\"\n",
        "        SELECT Album.Title, Artist.Name\n",
        "        FROM Album\n",
        "        JOIN Artist ON Album.ArtistId = Artist.ArtistId\n",
        "        WHERE Artist.Name LIKE '%{artist}%';\n",
        "        \"\"\",\n",
        "        include_columns=True\n",
        "    )\n",
        "\n",
        "@tool\n",
        "def get_tracks_by_artist(artist: str):\n",
        "    \"\"\"Get songs by an artist (or similar artists).\"\"\"\n",
        "    # Execute a SQL query to find tracks (songs) by a given artist, or similar artists.\n",
        "    # It joins Album, Artist, and Track tables to get song names and artist names.\n",
        "    return db.run(\n",
        "        f\"\"\"\n",
        "        SELECT Track.Name as SongName, Artist.Name as ArtistName\n",
        "        FROM Album\n",
        "        LEFT JOIN Artist ON Album.ArtistId = Artist.ArtistId\n",
        "        LEFT JOIN Track ON Track.AlbumId = Album.AlbumId\n",
        "        WHERE Artist.Name LIKE '%{artist}%';\n",
        "        \"\"\",\n",
        "        include_columns=True\n",
        "    )\n",
        "\n",
        "@tool\n",
        "def get_songs_by_genre(genre: str):\n",
        "    \"\"\"\n",
        "    Fetch songs from the database that match a specific genre.\n",
        "\n",
        "    Args:\n",
        "        genre (str): The genre of the songs to fetch.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of songs that match the specified genre.\n",
        "    \"\"\"\n",
        "    # First, find the GenreId for the given genre name.\n",
        "    genre_id_query = f\"SELECT GenreId FROM Genre WHERE Name LIKE '%{genre}%'\"\n",
        "    genre_ids = db.run(genre_id_query)\n",
        "\n",
        "    # If no genre IDs are found, return an informative message.\n",
        "    if not genre_ids:\n",
        "        return f\"No songs found for the genre: {genre}\"\n",
        "\n",
        "    # Safely evaluate the string result from db.run to get a list of tuples.\n",
        "    genre_ids = ast.literal_eval(genre_ids)\n",
        "    # Extract just the GenreId values and join them into a comma-separated string for the IN clause.\n",
        "    genre_id_list = \", \".join(str(gid[0]) for gid in genre_ids)\n",
        "\n",
        "    # Construct the query to get songs for the found genre IDs.\n",
        "    # It joins Track, Album, and Artist tables and limits the results to 8.\n",
        "    songs_query = f\"\"\"\n",
        "        SELECT Track.Name as SongName, Artist.Name as ArtistName\n",
        "        FROM Track\n",
        "        LEFT JOIN Album ON Track.AlbumId = Album.AlbumId\n",
        "        LEFT JOIN Artist ON Album.ArtistId = Artist.ArtistId\n",
        "        WHERE Track.GenreId IN ({genre_id_list})\n",
        "        GROUP BY Artist.Name\n",
        "        LIMIT 8;\n",
        "    \"\"\"\n",
        "    songs = db.run(songs_query, include_columns=True)\n",
        "\n",
        "    # If no songs are found for the genre, return an informative message.\n",
        "    if not songs:\n",
        "        return f\"No songs found for the genre: {genre}\"\n",
        "\n",
        "    # Safely evaluate the string result and format it into a list of dictionaries.\n",
        "    formatted_songs = ast.literal_eval(songs)\n",
        "    return [\n",
        "        {\"Song\": song[\"SongName\"], \"Artist\": song[\"ArtistName\"]}\n",
        "        for song in formatted_songs\n",
        "    ]\n",
        "\n",
        "@tool\n",
        "def check_for_songs(song_title):\n",
        "    \"\"\"Check if a song exists by its name.\"\"\"\n",
        "    # Execute a SQL query to check for the existence of a song by its title.\n",
        "    return db.run(\n",
        "        f\"\"\"\n",
        "        SELECT * FROM Track WHERE Name LIKE '%{song_title}%';\n",
        "        \"\"\",\n",
        "        include_columns=True\n",
        "    )\n",
        "\n",
        "# Aggregate all music-related tools into a list.\n",
        "music_tools = [get_albums_by_artist, get_tracks_by_artist, get_songs_by_genre, check_for_songs]\n",
        "\n",
        "# Bind the tools to our ChatOpenAI model.\n",
        "# This step configures the LLM so it knows about the available tools and their schemas,\n",
        "# allowing it to generate tool calls when appropriate based on the user's query.\n",
        "llm_with_music_tools = llm.bind_tools(music_tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UXTwPybqfC0"
      },
      "source": [
        "#### Nodes\n",
        "\n",
        "For our ReAct agent, we'll define two primary types of nodes:\n",
        "\n",
        "1.  **`music_assistant` (Reasoning Node)**: This node is an LLM responsible for `reasoning`. It takes the current conversation history and user query, considers the available tools, and decides the next best action. This could be to invoke a tool, or if the query is satisfied, to generate a final response.\n",
        "2.  **`music_tool_node` (Acting Node)**: This node is responsible for `acting`. When the `music_assistant` decides to use a tool, the `music_tool_node` receives the tool call, executes the specified tool function, and then returns the tool's output back to the graph state. LangGraph provides a convenient `ToolNode` utility that automatically handles the execution of tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQW1lBP-qfC0"
      },
      "outputs": [],
      "source": [
        "# Create a ToolNode instance. This node will automatically execute any tool calls\n",
        "# generated by an LLM that is bound to these tools.\n",
        "music_tool_node = ToolNode(music_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3j-i9FJqfC0"
      },
      "outputs": [],
      "source": [
        "# Define the system prompt for the music assistant.\n",
        "# This prompt provides instructions and persona for the LLM.\n",
        "# It emphasizes the agent's role, core responsibilities, and search guidelines.\n",
        "# The `memory` placeholder allows us to inject user preferences from long-term memory.\n",
        "def generate_music_assistant_prompt(memory: str = \"None\") -> str:\n",
        "    return f\"\"\"\n",
        "    You are a member of the assistant team, your role specifically is to focused on helping customers discover and learn about music in our digital catalog.\n",
        "    If you are unable to find playlists, songs, or albums associated with an artist, it is okay.\n",
        "    Just inform the customer that the catalog does not have any playlists, songs, or albums associated with that artist.\n",
        "    You also have context on any saved user preferences, helping you to tailor your response.\n",
        "\n",
        "    CORE RESPONSIBILITIES:\n",
        "    - Search and provide accurate information about songs, albums, artists, and playlists\n",
        "    - Offer relevant recommendations based on customer interests\n",
        "    - Handle music-related queries with attention to detail\n",
        "    - Help customers discover new music they might enjoy\n",
        "    - You are routed only when there are questions related to music catalog; ignore other questions.\n",
        "\n",
        "    SEARCH GUIDELINES:\n",
        "    1. Always perform thorough searches before concluding something is unavailable\n",
        "    2. If exact matches aren't found, try:\n",
        "       - Checking for alternative spellings\n",
        "       - Looking for similar artist names\n",
        "       - Searching by partial matches\n",
        "       - Checking different versions/remixes\n",
        "    3. When providing song lists:\n",
        "       - Include the artist name with each song\n",
        "       - Mention the album when relevant\n",
        "       - Note if it's part of any playlists\n",
        "       - Indicate if there are multiple versions\n",
        "\n",
        "    Additional context is provided below:\n",
        "\n",
        "    Prior saved user preferences: {memory}\n",
        "\n",
        "    Message history is also attached.\n",
        "    \"\"\"\n",
        "\n",
        "# Define the music_assistant node function.\n",
        "# This function receives the current `State` and `RunnableConfig`.\n",
        "def music_assistant(state: State, config: RunnableConfig):\n",
        "\n",
        "    # Fetch long-term memory (user preferences) from the state.\n",
        "    # If `loaded_memory` is not present in the state, default to \"None\".\n",
        "    memory = \"None\"\n",
        "    if \"loaded_memory\" in state:\n",
        "        memory = state[\"loaded_memory\"]\n",
        "\n",
        "    # Generate the system prompt for the music assistant, injecting the loaded memory.\n",
        "    music_assistant_prompt = generate_music_assistant_prompt(memory)\n",
        "\n",
        "    # Invoke the LLM (`llm_with_music_tools`) with the system prompt and the current message history.\n",
        "    # The LLM will decide whether to call a tool or generate a final response.\n",
        "    response = llm_with_music_tools.invoke([SystemMessage(music_assistant_prompt)] + state[\"messages\"])\n",
        "\n",
        "    # Update the state by appending the LLM's response to the `messages` list.\n",
        "    # The `add_messages` annotation in `State` ensures this is appended correctly.\n",
        "    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ6GvMc6qfC1"
      },
      "source": [
        "#### Edges\n",
        "For our ReAct agent, we need a **conditional edge** after the `music_assistant` node. This edge will determine:\n",
        "- If the `music_assistant` decided to invoke a tool, we should route to the `music_tool_node` to execute it.\n",
        "- If the `music_assistant` generated a final, human-readable response (i.e., no tool calls), we should `END` the sub-agent's execution, as the query is resolved.\n",
        "\n",
        "The `should_continue` function implements this conditional logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFgbgbILqfC1"
      },
      "outputs": [],
      "source": [
        "# Define a conditional edge function named `should_continue`.\n",
        "# This function determines the next step in the graph based on the LLM's response.\n",
        "def should_continue(state: State, config: RunnableConfig):\n",
        "    # Get the list of messages from the current state.\n",
        "    messages = state[\"messages\"]\n",
        "    # Get the last message, which is the response from the `music_assistant` LLM.\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    # Check if the last message contains any tool calls.\n",
        "    # LLMs generate `tool_calls` when they decide to use a function.\n",
        "    if not last_message.tool_calls:\n",
        "        # If there are no tool calls, it means the LLM has generated a final answer.\n",
        "        # In this case, the sub-agent's work is done, so we return \"end\" to signal completion.\n",
        "        return \"end\"\n",
        "    # Otherwise, if there are tool calls,\n",
        "    else:\n",
        "        # We need to execute the tool(s). So, we return \"continue\" to route to the tool execution node.\n",
        "        return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTYxxBAqfC1"
      },
      "source": [
        "#### Compile Graph!\n",
        "The `StateGraph` class is used to define the structure of our agent. We add nodes and edges, and then `compile` the graph. Compilation turns the defined graph into a runnable object, ready to be invoked.\n",
        "\n",
        "Key methods used:\n",
        "*   `StateGraph(State)`: Initializes a graph with our defined state schema.\n",
        "*   `add_node(name, node_function)`: Adds a node to the graph, associating a name with a callable Python function.\n",
        "*   `add_edge(source, target)`: Creates a direct, unconditional edge from `source` node to `target` node.\n",
        "*   `add_conditional_edges(source, condition_function, mapping)`: Creates a dynamic edge. `condition_function` is called to determine the next node based on its return value, which must match a key in the `mapping` dictionary.\n",
        "    *   `START`: A special entry point that signifies the beginning of the graph execution.\n",
        "    *   `END`: A special exit point that signifies the completion of the graph execution.\n",
        "*   `compile(name, checkpointer, store)`: Finalizes the graph.\n",
        "    *   `name`: A unique identifier for the compiled graph.\n",
        "    *   `checkpointer`: The short-term memory mechanism (`MemorySaver`) to persist and resume graph state.\n",
        "    *   `store`: The long-term memory mechanism (`InMemoryStore`) for data persistent across sessions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "import nest_asyncio\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# List of URLs containing LangGraph documentation\n",
        "# These URLs cover tutorials, concepts, and guides for the LangGraph framework\n",
        "LANGGRAPH_DOCS = [\n",
        "    \"https://langchain-ai.github.io/langgraph/\",  # Main documentation page\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/\",  # Customer support tutorial\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/chatbots/information-gather-prompting/\",  # Chatbot tutorial\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/\",  # Code assistant tutorial\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/\",  # Multi-agent collaboration\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/\",  # Agent supervisor pattern\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/\",  # Hierarchical agent teams\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/plan-and-execute/plan-and-execute/\",  # Plan and execute pattern\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/rewoo/rewoo/\",  # ReWOO (Reasoning WithOut Observation) tutorial\n",
        "    \"https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/\",  # LLM Compiler tutorial\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/high_level/\",  # High-level concepts\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/low_level/\",  # Low-level concepts\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/\",  # Agentic concepts\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\",  # Human-in-the-loop concepts\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/multi_agent/\",  # Multi-agent concepts\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/persistence/\",  # Persistence concepts\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/streaming/\",  # Streaming concepts\n",
        "    \"https://langchain-ai.github.io/langgraph/concepts/faq/\"  # Frequently asked questions\n",
        "]\n",
        "\n",
        "def get_langgraph_docs_retriever():\n",
        "    \"\"\"\n",
        "    Loads or creates a retriever for LangGraph documentation using a persisted Chroma vectorstore.\n",
        "\n",
        "    This function implements a caching mechanism:\n",
        "    1. If a vectorstore already exists on disk, it loads and returns it\n",
        "    2. If no vectorstore exists, it downloads the documentation, creates embeddings,\n",
        "       stores them in a vectorstore, and persists it to disk for future use\n",
        "\n",
        "    Returns:\n",
        "        Retriever: A retriever object for querying the LangGraph documentation using similarity search\n",
        "    \"\"\"\n",
        "    # Check if the vectorstore directory already exists on disk\n",
        "    # This allows us to skip the expensive document loading and embedding process\n",
        "    if os.path.exists(\"langgraph-docs-db\"):\n",
        "        print(\"Loading vectorstore from disk...\")\n",
        "        # Load the existing vectorstore from the persistent directory\n",
        "        vectorstore = Chroma(\n",
        "            collection_name=\"langgraph-docs\",  # Name of the collection in the vectorstore\n",
        "            embedding_function=Embedding_model,  # Embedding model for query encoding\n",
        "            persist_directory=\"langgraph-docs-db\"  # Directory where the vectorstore is saved\n",
        "        )\n",
        "        # Return a retriever interface for the vectorstore\n",
        "        # lambda_mult=0 disables the MMR (Maximum Marginal Relevance) diversity filter\n",
        "        return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "    # If vectorstore doesn't exist, create it from scratch\n",
        "    print(\"Creating new vectorstore from documentation...\")\n",
        "\n",
        "    # Download and load documents from each URL in LANGGRAPH_DOCS\n",
        "    # WebBaseLoader fetches the content from web pages and converts them to Document objects\n",
        "    docs = [WebBaseLoader(url).load() for url in LANGGRAPH_DOCS]\n",
        "\n",
        "    # Flatten the list of lists into a single list of documents\n",
        "    # Each WebBaseLoader.load() returns a list, so we need to flatten the nested structure\n",
        "    docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "    # Split documents into smaller chunks for better retrieval performance\n",
        "    # Smaller chunks allow for more precise similarity matching\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=200,  # Maximum tokens per chunk (small chunks for precise retrieval)\n",
        "        chunk_overlap=0  # No overlap between chunks to avoid redundancy\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "    # Create a new Chroma vectorstore with the specified configuration\n",
        "    vectorstore = Chroma(\n",
        "        collection_name=\"langgraph-docs\",  # Name of the collection\n",
        "        embedding_function=Embedding_model,  # Embedding model for converting text to vectors\n",
        "        persist_directory=\"langgraph-docs-db\"  # Directory to persist the vectorstore\n",
        "    )\n",
        "\n",
        "    # Add the split documents to the vectorstore\n",
        "    # This creates embeddings for each chunk and stores them in the vector database\n",
        "    vectorstore.add_documents(doc_splits)\n",
        "    print(\"Vectorstore created and persisted to disk\")\n",
        "\n",
        "    # Return a retriever interface for the new vectorstore\n",
        "    return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "def show_graph(graph, xray=False):\n",
        "    \"\"\"\n",
        "    Display a LangGraph mermaid diagram with fallback rendering.\n",
        "\n",
        "    This function attempts to render a LangGraph as a visual diagram using Mermaid.\n",
        "    It includes error handling to fall back to an alternative renderer if the default fails.\n",
        "\n",
        "    Args:\n",
        "        graph: The LangGraph object that has a get_graph() method for visualization\n",
        "        xray (bool): Whether to show internal graph details in xray mode\n",
        "\n",
        "    Returns:\n",
        "        Image: An IPython Image object containing the rendered graph diagram\n",
        "    \"\"\"\n",
        "    from IPython.display import Image\n",
        "\n",
        "    try:\n",
        "        # Try the default mermaid renderer first (uses mermaid.ink service)\n",
        "        # This is the fastest option but may fail due to network issues or service unavailability\n",
        "        return Image(graph.get_graph(xray=xray).draw_mermaid_png())\n",
        "    except Exception as e:\n",
        "        # If the default renderer fails, fall back to pyppeteer\n",
        "        # pyppeteer uses a local headless Chrome instance to render the diagram\n",
        "        print(f\"Default renderer failed ({e}), falling back to pyppeteer...\")\n",
        "\n",
        "        # Apply nest_asyncio to handle async operations in Jupyter environments\n",
        "        # This is necessary because pyppeteer uses async operations\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "\n",
        "        # Import the MermaidDrawMethod enum for specifying the draw method\n",
        "        from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "\n",
        "        # Use pyppeteer as the drawing method (local rendering)\n",
        "        return Image(graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER))"
      ],
      "metadata": {
        "id": "FjZZAzzBxByJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2xLhW4SqfC1"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END # Core LangGraph classes and special node names\n",
        " # Utility function to visualize the graph (assumed to be in a utils.py file)\n",
        "\n",
        "# Initialize a StateGraph with our defined `State` schema.\n",
        "# This tells LangGraph how the data will flow and be managed within the graph.\n",
        "music_workflow = StateGraph(State)\n",
        "\n",
        "# Add the 'music_assistant' node to the graph.\n",
        "# This node is responsible for the LLM's reasoning and generating tool calls or final responses.\n",
        "music_workflow.add_node(\"music_assistant\", music_assistant)\n",
        "\n",
        "# Add the 'music_tool_node' to the graph.\n",
        "# This node is responsible for executing the tools when requested by the LLM.\n",
        "music_workflow.add_node(\"music_tool_node\", music_tool_node)\n",
        "\n",
        "\n",
        "# Define the starting point of the graph.\n",
        "# All queries will initially enter the 'music_assistant' node.\n",
        "music_workflow.add_edge(START, \"music_assistant\")\n",
        "\n",
        "# Add a conditional edge from 'music_assistant'.\n",
        "# The `should_continue` function will be called to determine the next node.\n",
        "music_workflow.add_conditional_edges(\n",
        "    \"music_assistant\", # Source node\n",
        "    should_continue,   # Conditional function to call\n",
        "    {\n",
        "        # If `should_continue` returns \"continue\", route to `music_tool_node`.\n",
        "        \"continue\": \"music_tool_node\",\n",
        "        # If `should_continue` returns \"end\", terminate the graph execution.\n",
        "        \"end\": END,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Add a normal edge from 'music_tool_node' back to 'music_assistant'.\n",
        "# After a tool is executed, the result is fed back to the LLM for further reasoning\n",
        "# or to formulate a final response (ReAct loop).\n",
        "music_workflow.add_edge(\"music_tool_node\", \"music_assistant\")\n",
        "\n",
        "# Compile the graph into a runnable object.\n",
        "# `name`: A unique identifier for this compiled graph (useful for debugging and logging).\n",
        "# `checkpointer`: The short-term memory mechanism (MemorySaver) for thread-specific state.\n",
        "# `store`: The long-term memory mechanism (InMemoryStore) for persistent user data.\n",
        "music_catalog_subagent = music_workflow.compile(name=\"music_catalog_subagent\", checkpointer=checkpointer, store = in_memory_store)\n",
        "\n",
        "# Display a visualization of the compiled graph.\n",
        "show_graph(music_catalog_subagent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBMUOlrsqfC2"
      },
      "source": [
        "#### Testing the Music Catalog Sub-Agent\n",
        "\n",
        "Now that our first ReAct sub-agent is compiled, let's test its functionality. We'll simulate a customer inquiry and observe how the agent processes it, potentially invoking tools and providing a response.\n",
        "\n",
        "Key elements in testing:\n",
        "*   `uuid.uuid4()`: Generates a unique thread ID for each conversation. This is crucial for the checkpointer to maintain separate states for different conversations.\n",
        "    *   In a real application, this `thread_id` would typically correspond to a user session or a unique conversation identifier.\n",
        "*   `config`: A dictionary passed to the `invoke` method, containing configurable parameters like the `thread_id`. The checkpointer uses this `thread_id` to load and save the correct state.\n",
        "*   `invoke()`: Starts the execution of the LangGraph. It takes the initial input (`messages` in our `State`) and the `config`.\n",
        "*   `pretty_print()`: A utility method (assuming it's defined for `AnyMessage` or similar) to display the messages in a readable format, showing roles and content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xqER9kFBqfC2"
      },
      "outputs": [],
      "source": [
        "import uuid # Module for generating unique identifiers\n",
        "\n",
        "# Generate a unique thread ID for this conversation.\n",
        "# This ensures that the conversation state is isolated and can be resumed later.\n",
        "thread_id = uuid.uuid4()\n",
        "\n",
        "# Define the customer's question.\n",
        "question = \"I like the Rolling Stones. What songs do you recommend by them or by other artists that I might like?\"\n",
        "\n",
        "# Create the configuration dictionary for invoking the graph.\n",
        "# The `thread_id` is essential for the checkpointer to manage state.\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Invoke the `music_catalog_subagent` with the initial human message and configuration.\n",
        "# The `invoke` method runs the graph to completion and returns the final state.\n",
        "result = music_catalog_subagent.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
        "\n",
        "# Iterate through the messages in the final state and print them for observation.\n",
        "# `pretty_print()` provides a formatted output of the message content and role.\n",
        "for message in result[\"messages\"]:\n",
        "   message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrR5aLv8qfC2"
      },
      "source": [
        "### 1.2. Building ReAct Agent using LangGraph Pre-built: The Invoice Information Sub-Agent\n",
        "\n",
        "While building agents from scratch provides deep understanding, LangGraph also offers **pre-built libraries** for common architectures. These pre-built solutions allow for rapid development of standard agent patterns like ReAct, saving significant boilerplate code.\n",
        "\n",
        "In this section, we will demonstrate how to create our second sub-agent, the **Invoice Information Sub-Agent**, using the `create_react_agent` pre-built utility. This agent will be responsible for handling all customer inquiries related to invoices and past purchases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ud4FlbSqfC2"
      },
      "source": [
        "#### Defining Tools and Prompt for Invoice Sub-Agent\n",
        "\n",
        "Similar to our music sub-agent, the invoice sub-agent requires its own set of specialized tools and a tailored prompt. These tools will interact with the Chinook database to retrieve invoice-specific information.\n",
        "\n",
        "Each tool is designed to answer a specific type of query a customer might have about their invoices. The prompts will guide the LLM to understand its role and effectively use these tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0wfW1DmqfC2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "@tool\n",
        "def get_invoices_by_customer_sorted_by_date(customer_id: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Look up all invoices for a customer using their ID.\n",
        "    The invoices are sorted in descending order by invoice date, which helps when the customer wants to view their most recent/oldest invoice, or if\n",
        "    they want to view invoices within a specific date range.\n",
        "\n",
        "    Args:\n",
        "        customer_id (str): customer_id, which serves as the identifier.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of invoices for the customer.\n",
        "    \"\"\"\n",
        "    # Executes a SQL query to retrieve all invoice details for a given customer ID,\n",
        "    # ordered by invoice date in descending order (most recent first).\n",
        "    return db.run(f\"SELECT * FROM Invoice WHERE CustomerId = {customer_id} ORDER BY InvoiceDate DESC;\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_invoices_sorted_by_unit_price(customer_id: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Use this tool when the customer wants to know the details of one of their invoices based on the unit price/cost of the invoice.\n",
        "    This tool looks up all invoices for a customer, and sorts the unit price from highest to lowest. In order to find the invoice associated with the customer,\n",
        "    we need to know the customer ID.\n",
        "\n",
        "    Args:\n",
        "        customer_id (str): customer_id, which serves as the identifier.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of invoices sorted by unit price.\n",
        "    \"\"\"\n",
        "    # Executes a SQL query to retrieve invoice details along with the unit price of items in those invoices,\n",
        "    # for a given customer ID, ordered by unit price in descending order (highest unit price first).\n",
        "    query = f\"\"\"\n",
        "        SELECT Invoice.*, InvoiceLine.UnitPrice\n",
        "        FROM Invoice\n",
        "        JOIN InvoiceLine ON Invoice.InvoiceId = InvoiceLine.InvoiceId\n",
        "        WHERE Invoice.CustomerId = {customer_id}\n",
        "        ORDER BY InvoiceLine.UnitPrice DESC;\n",
        "    \"\"\"\n",
        "    return db.run(query)\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_employee_by_invoice_and_customer(invoice_id: str, customer_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    This tool will take in an invoice ID and a customer ID and return the employee information associated with the invoice.\n",
        "\n",
        "    Args:\n",
        "        invoice_id (int): The ID of the specific invoice.\n",
        "        customer_id (str): customer_id, which serves as the identifier.\n",
        "\n",
        "    Returns:\n",
        "        dict: Information about the employee associated with the invoice.\n",
        "    \"\"\"\n",
        "\n",
        "    # Executes a SQL query to find the employee associated with a specific invoice and customer.\n",
        "    # It joins Employee, Customer, and Invoice tables to retrieve employee first name, title, and email.\n",
        "    query = f\"\"\"\n",
        "        SELECT Employee.FirstName, Employee.Title, Employee.Email\n",
        "        FROM Employee\n",
        "        JOIN Customer ON Customer.SupportRepId = Employee.EmployeeId\n",
        "        JOIN Invoice ON Invoice.CustomerId = Customer.CustomerId\n",
        "        WHERE Invoice.InvoiceId = ({invoice_id}) AND Invoice.CustomerId = ({customer_id});\n",
        "    \"\"\"\n",
        "\n",
        "    employee_info = db.run(query, include_columns=True)\n",
        "\n",
        "    # Checks if any employee information was found.\n",
        "    if not employee_info:\n",
        "        return f\"No employee found for invoice ID {invoice_id} and customer identifier {customer_id}.\"\n",
        "    return employee_info\n",
        "\n",
        "# Aggregate all invoice-related tools into a list.\n",
        "invoice_tools = [get_invoices_by_customer_sorted_by_date, get_invoices_sorted_by_unit_price, get_employee_by_invoice_and_customer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVRFeTRaqfC2"
      },
      "outputs": [],
      "source": [
        "# Define the system prompt for the invoice information sub-agent.\n",
        "# This prompt sets the persona and core responsibilities for the LLM within this sub-agent's domain.\n",
        "# It explicitly lists the tools available to this agent and provides guidelines for their use.\n",
        "invoice_subagent_prompt = \"\"\"\n",
        "    You are a subagent among a team of assistants. You are specialized for retrieving and processing invoice information. You are routed for invoice-related portion of the questions, so only respond to them..\n",
        "\n",
        "    You have access to three tools. These tools enable you to retrieve and process invoice information from the database. Here are the tools:\n",
        "    - get_invoices_by_customer_sorted_by_date: This tool retrieves all invoices for a customer, sorted by invoice date.\n",
        "    - get_invoices_sorted_by_unit_price: This tool retrieves all invoices for a customer, sorted by unit price.\n",
        "    - get_employee_by_invoice_and_customer: This tool retrieves the employee information associated with an invoice and a customer.\n",
        "\n",
        "    If you are unable to retrieve the invoice information, inform the customer you are unable to retrieve the information, and ask if they would like to search for something else.\n",
        "\n",
        "    CORE RESPONSIBILITIES:\n",
        "    - Retrieve and process invoice information from the database\n",
        "    - Provide detailed information about invoices, including customer details, invoice dates, total amounts, employees associated with the invoice, etc. when the customer asks for it.\n",
        "    - Always maintain a professional, friendly, and patient demeanor\n",
        "\n",
        "    You may have additional context that you should use to help answer the customer's query. It will be provided to you below:\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCXsUr9cqfC3"
      },
      "source": [
        "#### Using the Pre-built Library: `create_react_agent`\n",
        "\n",
        "The `create_react_agent` function handles:\n",
        "*   Binding the provided `tools` to the `model`.\n",
        "*   Setting up the LLM (`model`) as the reasoning node.\n",
        "*   Setting up a `ToolNode` to execute the tools.\n",
        "*   Defining the conditional logic (edges) to loop between the LLM and the tools until a final answer is produced or a recursion limit is reached.\n",
        "\n",
        "We provide it with:\n",
        "*   `model`: The LLM to use (our `ChatOpenAI` instance).\n",
        "*   `tools`: The list of functions available to the agent.\n",
        "*   `name`: A unique name for this agent (useful for identification in a multi-agent system).\n",
        "*   `prompt`: The system prompt to guide the LLM's behavior.\n",
        "*   `state_schema`: The `State` class we defined, ensuring consistency across agents.\n",
        "*   `checkpointer` and `store`: Our memory mechanisms for thread-level state and long-term user data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv8l2TFSqfC3"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent # Import the pre-built ReAct agent creator\n",
        "\n",
        "# Define the invoice information subagent using the pre-built `create_react_agent`.\n",
        "# This function internally sets up the nodes (LLM and ToolNode) and edges for a ReAct loop.\n",
        "invoice_information_subagent = create_react_agent(\n",
        "    llm,                          # The language model to use for reasoning\n",
        "    tools=invoice_tools,            # The list of tools available to this agent\n",
        "    name=\"invoice_information_subagent\", # A unique name for this agent within the graph\n",
        "    prompt=invoice_subagent_prompt, # The system prompt for this agent's persona and instructions\n",
        "    state_schema=State,             # The shared state schema for the graph\n",
        "    checkpointer=checkpointer,      # The checkpointer for short-term (thread-level) memory\n",
        "    store = in_memory_store         # The in-memory store for long-term user data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmiRYCfQqfC3"
      },
      "source": [
        "#### Testing the Invoice Information Sub-Agent!\n",
        "\n",
        "Let's test our newly created invoice sub-agent to ensure it correctly processes queries related to invoices and utilizes its specific tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "o0OnCmnzqfC3"
      },
      "outputs": [],
      "source": [
        "thread_id = uuid.uuid4() # Generate a new unique thread ID for this test conversation.\n",
        "\n",
        "# Define a sample question for the invoice sub-agent.\n",
        "question = \"My customer id is 1. What was my most recent invoice, and who was the employee that helped me with it?\"\n",
        "\n",
        "# Set up the configuration with the thread ID.\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Invoke the invoice sub-agent with the question and configuration.\n",
        "result = invoice_information_subagent.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
        "\n",
        "# Print the conversation history from the result for verification.\n",
        "for message in result[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnDiqEBqqfC3"
      },
      "source": [
        "## Part 2: Building Multi-Agent Architecture (Supervisor)\n",
        "\n",
        "### The Supervisor Role\n",
        "\n",
        "\n",
        "The supervisor's primary responsibilities include:\n",
        "*   **Intent Recognition**: Analyzing the incoming customer query to determine its primary intent (e.g., music-related, invoice-related, or something else).\n",
        "    *   `music_catalog_information_subagent`: Handles queries about songs, albums, artists, genres, and music preferences.\n",
        "    *   `invoice_information_subagent`: Handles queries about past purchases, invoices, and billing details.\n",
        "*   **Routing**: Directing the query to the appropriate specialized sub-agent based on the recognized intent.\n",
        "*   **Orchestration (Implicit)**: While the supervisor explicitly *routes*, it implicitly orchestrates by ensuring the right agent is active at the right time. For multi-turn conversations, it might re-route or allow the current sub-agent to continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "198mGi03qfC4"
      },
      "source": [
        "First, we will create a set of instructions for our supervisor. This prompt defines the supervisor's persona, its role as a planner and router, and the capabilities of the sub-agents it oversees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "188glGpWqfC4"
      },
      "outputs": [],
      "source": [
        "supervisor_prompt = \"\"\"You are an expert customer support assistant for a digital music store.\n",
        "You are dedicated to providing exceptional service and ensuring customer queries are answered thoroughly.\n",
        "You have a team of subagents that you can use to help answer queries from customers.\n",
        "Your primary role is to serve as a supervisor/planner for this multi-agent team that helps answer queries from customers.\n",
        "\n",
        "Your team is composed of two subagents that you can use to help answer the customer's request:\n",
        "1. music_catalog_information_subagent: this subagent has access to user's saved music preferences. It can also retrieve information about the digital music store's music\n",
        "catalog (albums, tracks, songs, etc.) from the database.\n",
        "3. invoice_information_subagent: this subagent is able to retrieve information about a customer's past purchases or invoices\n",
        "from the database.\n",
        "\n",
        "Based on the existing steps that have been taken in the messages, your role is to generate the next subagent that needs to be called.\n",
        "This could be one step in an inquiry that needs multiple sub-agent calls. \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnM61YEBqfC4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create the supervisor workflow using the `create_supervisor` utility.\n",
        "# This function dynamically sets up the graph to route between the provided agents.\n",
        "supervisor_prebuilt_workflow = create_supervisor(\n",
        "    agents=[invoice_information_subagent, music_catalog_subagent], # List of sub-agents the supervisor can route to\n",
        "    output_mode=\"last_message\", # Specifies that the supervisor should output only the last message from the routed agent.\n",
        "                                # Alternative is \"full_history\" to get all messages from the sub-agent.\n",
        "    model=llm,                # The LLM to act as the supervisor (for routing decisions).\n",
        "    prompt=(supervisor_prompt), # The system prompt guiding the supervisor's behavior.\n",
        "    state_schema=State          # The shared state schema for the entire multi-agent graph.\n",
        ")\n",
        "\n",
        "# Compile the supervisor workflow into a runnable object.\n",
        "# This makes it ready for invocation and integrates it with our memory systems.\n",
        "supervisor_prebuilt = supervisor_prebuilt_workflow.compile(name=\"music_catalog_subagent\", checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "# Display a visualization of the compiled supervisor graph.\n",
        "# Notice how the supervisor acts as the central hub, directing traffic to its sub-agents.\n",
        "show_graph(supervisor_prebuilt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfl7XIBzqfC4"
      },
      "source": [
        "#### Testing the Multi-Agent Supervisor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lnmHupzxqfC4"
      },
      "outputs": [],
      "source": [
        "thread_id = uuid.uuid4() # Generate a fresh thread ID for this conversation.\n",
        "\n",
        "# Define a question that involves both invoice and music information.\n",
        "question = \"My customer ID is 1. How much was my most recent purchase? What albums do you have by U2?\"\n",
        "\n",
        "# Configure the invocation with the thread ID.\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Invoke the `supervisor_prebuilt` graph with the human message.\n",
        "# The supervisor will analyze the question, route it to the appropriate sub-agent(s),\n",
        "# and return the final response from the last active agent.\n",
        "result = supervisor_prebuilt.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
        "\n",
        "# Print the messages from the resulting state to see the conversation flow and final answer.\n",
        "for message in result[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDAr90RHqfC4"
      },
      "source": [
        "## Part 3: Adding Customer Verification through Human-in-the-Loop\n",
        "\n",
        "Currently, our agent assumes we have the `customer_id` readily available or directly provided in the initial query. In a realistic customer support scenario, we often need to **first verify the customer's identity** before processing sensitive inquiries like invoice details. This verification might involve asking the user for an identifier (e.g., email, phone, customer ID) and then looking it up in the database.\n",
        "\n",
        "To implement this, we'll introduce a **human-in-the-loop** component. This means the graph can *pause* execution and wait for additional input from the user (or a human agent) before proceeding. LangGraph's `interrupt` mechanism is perfect for this.\n",
        "\n",
        "\n",
        "In this step, we will add two new nodes to our workflow:\n",
        "\n",
        "*   **`verify_info` node**: This node will attempt to extract a customer identifier from the user's input and verify it against our database. If found, it updates the `customer_id` in the graph state. If not found or not provided, it prompts the user for the information.\n",
        "*   **`human_input` node**: This is a simple node that explicitly triggers an `interrupt`, pausing the graph execution until the user provides the necessary information to resume.\n",
        "\n",
        "We'll also leverage LangChain's structured output capabilities to reliably parse user input for identifiers using Pydantic schemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEPuiV53qfC4"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field # Pydantic for defining data schemas and field validation\n",
        "\n",
        "# Define a Pydantic BaseModel to structure the expected user input for account information.\n",
        "# This helps the LLM to parse specific entities (identifier) from free-form text.\n",
        "class UserInput(BaseModel):\n",
        "    \"\"\"Schema for parsing user-provided account information.\"\"\"\n",
        "    # `identifier` field: Expects a string, with a description for the LLM to understand its purpose.\n",
        "    identifier: str = Field(description = \"Identifier, which can be a customer ID, email, or phone number.\")\n",
        "\n",
        "# Bind the Pydantic schema to our LLM using `with_structured_output`.\n",
        "# This forces the LLM to generate output that conforms to the `UserInput` schema, making parsing reliable.\n",
        "structured_llm = llm.with_structured_output(schema=UserInput)\n",
        "\n",
        "# Define a system prompt specifically for the structured LLM.\n",
        "# This prompt instructs the LLM on how to extract the customer identifier from messages.\n",
        "structured_system_prompt = \"\"\"You are a customer service representative responsible for extracting customer identifier.\\n\n",
        "Only extract the customer's account information from the message history.\n",
        "If they haven't provided the information yet, return an empty string for the file\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWuVEuH3qfC5"
      },
      "outputs": [],
      "source": [
        "from typing import Optional # For type hinting optional values\n",
        "\n",
        "# Helper function to retrieve a customer ID from various identifiers (ID, phone, email).\n",
        "def get_customer_id_from_identifier(identifier: str) -> Optional[int]:\n",
        "    \"\"\"\n",
        "    Retrieve Customer ID using an identifier, which can be a customer ID, email, or phone number.\n",
        "\n",
        "    Args:\n",
        "        identifier (str): The identifier can be customer ID, email, or phone.\n",
        "\n",
        "    Returns:\n",
        "        Optional[int]: The CustomerId if found, otherwise None.\n",
        "    \"\"\"\n",
        "    # Check if the identifier is purely numeric, indicating a direct customer ID.\n",
        "    if identifier.isdigit():\n",
        "        return int(identifier)\n",
        "\n",
        "    # Check if the identifier starts with '+', suggesting a phone number.\n",
        "    elif identifier[0] == \"+\":\n",
        "        query = f\"SELECT CustomerId FROM Customer WHERE Phone = '{identifier}';\"\n",
        "        result = db.run(query)\n",
        "        formatted_result = ast.literal_eval(result) # Safely evaluate string to list/tuple\n",
        "        if formatted_result:\n",
        "            return formatted_result[0][0] # Return the first CustomerId found\n",
        "\n",
        "    # Check if the identifier contains '@', suggesting an email address.\n",
        "    elif \"@\" in identifier:\n",
        "        query = f\"SELECT CustomerId FROM Customer WHERE Email = '{identifier}';\"\n",
        "        result = db.run(query)\n",
        "        formatted_result = ast.literal_eval(result)\n",
        "        if formatted_result:\n",
        "            return formatted_result[0][0] # Return the first CustomerId found\n",
        "\n",
        "    # If no matching identifier type is found or no ID is retrieved, return None.\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR1RNyH6qfC5"
      },
      "outputs": [],
      "source": [
        "# Define the `verify_info` node function.\n",
        "# This node is responsible for verifying the customer's identity based on their input.\n",
        "def verify_info(state: State, config: RunnableConfig):\n",
        "    \"\"\"Verify the customer's account by parsing their input and matching it with the database.\"\"\"\n",
        "\n",
        "    # Check if a customer_id is already present in the state.\n",
        "    # If it is, verification is complete, and the node does nothing (passes).\n",
        "    if state.get(\"customer_id\") is None:\n",
        "        # System instructions for the verification LLM.\n",
        "        system_instructions = \"\"\"You are a music store agent, where you are trying to verify the customer identity\n",
        "        as the first step of the customer support process.\n",
        "        Only after their account is verified, you would be able to support them on resolving the issue.\n",
        "        In order to verify their identity, one of their customer ID, email, or phone number needs to be provided.\n",
        "        If the customer has not provided the information yet, please ask them for it.\n",
        "        If they have provided the identifier but cannot be found, please ask them to revise it.\"\"\"\n",
        "\n",
        "        # Get the most recent user message from the state.\n",
        "        user_input = state[\"messages\"][-1]\n",
        "\n",
        "        # Use the structured LLM to parse the user's input for an identifier.\n",
        "        # It combines the structured system prompt with the user's message.\n",
        "        parsed_info = structured_llm.invoke([SystemMessage(content=structured_system_prompt)] + [user_input])\n",
        "\n",
        "        # Extract the identified identifier string.\n",
        "        identifier = parsed_info.identifier\n",
        "\n",
        "        customer_id = \"\" # Initialize customer_id as an empty string.\n",
        "        # Attempt to find the customer ID in the database using the helper function.\n",
        "        if (identifier):\n",
        "            customer_id = get_customer_id_from_identifier(identifier)\n",
        "\n",
        "        # If a valid customer_id was found,\n",
        "        if customer_id != \"\":\n",
        "            # Create a system message confirming verification.\n",
        "            intent_message = SystemMessage(\n",
        "                content= f\"Thank you for providing your information! I was able to verify your account with customer id {customer_id}.\"\n",
        "            )\n",
        "            # Update the state with the found customer_id and the confirmation message.\n",
        "            return {\n",
        "                  \"customer_id\": customer_id,\n",
        "                  \"messages\" : [intent_message]\n",
        "                  }\n",
        "        # If no customer_id was found or provided,\n",
        "        else:\n",
        "          # Invoke the base LLM with instructions to prompt the user for their identifier or revise it.\n",
        "          response = llm.invoke([SystemMessage(content=system_instructions)]+state['messages'])\n",
        "          # Update the state with the LLM's response (the prompt for user input).\n",
        "          return {\"messages\": [response]}\n",
        "\n",
        "    else:\n",
        "        # If `customer_id` is already in state, this node does nothing.\n",
        "        # This `pass` implies that the graph will simply proceed to the next edge,\n",
        "        # as defined in the graph compilation.\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfV4u7fjqfDE"
      },
      "source": [
        "Now, let's create our `human_input` node. This node is designed to trigger an `interrupt` in the graph. When an interrupt occurs, the graph pauses, and control is returned to the caller (e.g., the notebook or an application). The caller can then choose to resume the graph, optionally providing new input.\n",
        "\n",
        "This is how we implement the human-in-the-loop interaction for customer verification: the graph asks for an identifier, pauses, and waits for the user to provide it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwinxMW4qfDE"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import interrupt # Import the `interrupt` function for pausing graph execution\n",
        "\n",
        "# Define the `human_input` node function.\n",
        "# This node serves as a placeholder to signal that human intervention is required.\n",
        "def human_input(state: State, config: RunnableConfig):\n",
        "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
        "    # `interrupt(\"Please provide input.\")` pauses the graph execution.\n",
        "    # The string message is passed as a reason for the interrupt.\n",
        "    # When the graph is resumed, the new input will be stored in `user_input`.\n",
        "    user_input = interrupt(\"Please provide input.\")\n",
        "\n",
        "    # The new user input (after resume) is then added to the messages in the state.\n",
        "    return {\"messages\": [user_input]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeXZWkkeqfDE"
      },
      "source": [
        "Let's put this together! We'll integrate the `verify_info` and `human_input` nodes into a new graph structure. The flow will be:\n",
        "\n",
        "1.  **`START`** -> **`verify_info`**: All incoming queries first attempt to verify the customer.\n",
        "2.  **`verify_info` (Conditional Edge)**:\n",
        "    *   If `customer_id` is *not* found (meaning verification failed or input is pending), it routes to **`human_input`**.\n",
        "    *   If `customer_id` *is* found (meaning verification succeeded), it routes to the **`supervisor`**.\n",
        "3.  **`human_input`** -> **`verify_info`**: After the user provides input to resume the graph, it routes back to `verify_info` to re-attempt verification with the new input.\n",
        "4.  **`supervisor`** -> **`END`**: Once the main query is handled by the supervisor and its sub-agents, the graph ends.\n",
        "\n",
        "This setup ensures that customer identity is verified before any other action, and gracefully handles cases where identity needs to be provided or re-attempted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFXusGfXqfDE"
      },
      "outputs": [],
      "source": [
        "# Define the conditional edge function for `verify_info`.\n",
        "# This function checks if a `customer_id` has been successfully set in the state.\n",
        "def should_interrupt(state: State, config: RunnableConfig):\n",
        "    # If `customer_id` is present, it means verification was successful or already done, so continue.\n",
        "    if state.get(\"customer_id\") is not None:\n",
        "        return \"continue\"\n",
        "    # Otherwise, it means customer ID is missing or couldn't be verified, so interrupt for human input.\n",
        "    else:\n",
        "        return \"interrupt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs3PLnl4qfDF"
      },
      "outputs": [],
      "source": [
        "# Initialize a new StateGraph for our multi-agent system with human-in-the-loop verification.\n",
        "multi_agent_verify = StateGraph(State)\n",
        "\n",
        "# Add the `verify_info` node for customer identity verification.\n",
        "multi_agent_verify.add_node(\"verify_info\", verify_info)\n",
        "\n",
        "# Add the `human_input` node, which triggers an interrupt to get user input.\n",
        "multi_agent_verify.add_node(\"human_input\", human_input)\n",
        "\n",
        "# Add the `supervisor` node, which orchestrates the sub-agents for query handling.\n",
        "multi_agent_verify.add_node(\"supervisor\", supervisor_prebuilt)\n",
        "\n",
        "# Define the entry point: all interactions start with customer verification.\n",
        "multi_agent_verify.add_edge(START, \"verify_info\")\n",
        "\n",
        "# Define the conditional routing after `verify_info`.\n",
        "# `should_interrupt` decides whether to continue to the supervisor or prompt for human input.\n",
        "multi_agent_verify.add_conditional_edges(\n",
        "    \"verify_info\",     # Source node\n",
        "    should_interrupt,  # Conditional function\n",
        "    {\n",
        "        # If verification is successful, continue to the main supervisor agent.\n",
        "        \"continue\": \"supervisor\",\n",
        "        # If verification is needed (or failed), route to `human_input` to prompt the user.\n",
        "        \"interrupt\": \"human_input\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# After `human_input` (once resumed), loop back to `verify_info` to try verification again.\n",
        "multi_agent_verify.add_edge(\"human_input\", \"verify_info\")\n",
        "\n",
        "# The supervisor is the final processing stage before the graph ends.\n",
        "multi_agent_verify.add_edge(\"supervisor\", END)\n",
        "\n",
        "# Compile the complete graph, integrating all nodes and edges with our memory systems.\n",
        "multi_agent_verify_graph = multi_agent_verify.compile(name=\"multi_agent_verify\", checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "# Display the visualization of the new graph.\n",
        "show_graph(multi_agent_verify_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOm5cX3AqfDF"
      },
      "source": [
        "#### Testing the Human-in-the-Loop Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hCromuIqfDF"
      },
      "outputs": [],
      "source": [
        "thread_id = uuid.uuid4() # Generate a new unique thread ID.\n",
        "\n",
        "# Initial question without providing customer ID.\n",
        "question = \"How much was my most recent purchase?\"\n",
        "\n",
        "# Configuration for the graph invocation.\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Invoke the graph. This first invocation should hit the `human_input` node and interrupt.\n",
        "result = multi_agent_verify_graph.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
        "\n",
        "# Print messages to observe the agent asking for the customer ID.\n",
        "for message in result[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1Xbf9OGHqfDF"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import Command # Import Command for resuming graph execution\n",
        "\n",
        "# Now, we simulate the user providing their phone number to resume the conversation.\n",
        "question = \"My phone number is +55 (12) 3923-5555.\"\n",
        "\n",
        "# Resume from the interrupt using `Command(resume=...)`.\n",
        "# The `resume` argument carries the new user input, which gets processed by `human_input` node\n",
        "# and then passed back to `verify_info`.\n",
        "# The `config` must be the same as the initial invocation to resume the correct thread.\n",
        "result = multi_agent_verify_graph.invoke(Command(resume=question), config=config)\n",
        "\n",
        "# Print the conversation messages to see the verification and subsequent processing.\n",
        "for message in result[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i992AwDFqfDF"
      },
      "source": [
        "Now, if we ask a follow-up question within the *same thread*, our agent state (managed by the checkpointer) will *already store our `customer_id`*. This means the `verify_info` node will simply `pass` without re-prompting, and the query will be routed directly to the supervisor, demonstrating the benefit of short-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "w243FOGoqfDF"
      },
      "outputs": [],
      "source": [
        "# Follow-up question in the same thread (using the same `thread_id`).\n",
        "question = \"What albums do you have by the Rolling Stones?\"\n",
        "\n",
        "# Invoke the graph again. Since the `customer_id` is already in the state,\n",
        "# the verification step will be skipped, and the query will directly go to the supervisor.\n",
        "result = multi_agent_verify_graph.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
        "\n",
        "# Print the results. You should see the music catalog sub-agent's response directly.\n",
        "for message in result[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5DZZEfTqfDF"
      },
      "source": [
        "## Part 4: Adding Long-Term Memory (User Preferences)\n",
        "\n",
        "In this step, we will add two new nodes to manage user preferences related to music:\n",
        "\n",
        "*   **`load_memory` node**: This node will load any existing music preferences associated with the verified `customer_id` from our `InMemoryStore` (our long-term memory store) into the current graph `State` (`loaded_memory`). This ensures the agent has relevant context for personalized responses.\n",
        "*   **`create_memory` node**: After the main query is handled, this node will analyze the conversation that just took place. If the customer shared any new music interests, it will update or create a `UserProfile` in the `InMemoryStore`, saving these preferences for future interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv7oWu8vqfDG"
      },
      "outputs": [],
      "source": [
        "from langgraph.store.base import BaseStore # Base class for defining custom stores for LangGraph\n",
        "\n",
        "# Helper function to format user memory (music preferences) into a readable string.\n",
        "def format_user_memory(user_data):\n",
        "    \"\"\"Formats music preferences from users, if available.\"\"\"\n",
        "    profile = user_data['memory'] # Access the 'memory' key from the stored dictionary\n",
        "    result = \"\" # Initialize an empty string for the formatted result\n",
        "\n",
        "    # Check if the profile object has a 'music_preferences' attribute and if it's not empty.\n",
        "    if hasattr(profile, 'music_preferences') and profile.music_preferences:\n",
        "        # If preferences exist, join them into a comma-separated string.\n",
        "        result += f\"Music Preferences: {', '.join(profile.music_preferences)}\"\n",
        "\n",
        "    return result.strip() # Return the formatted string, removing any leading/trailing whitespace.\n",
        "\n",
        "# Define the `load_memory` node function.\n",
        "# This node loads a user's long-term memory (music preferences) into the current state.\n",
        "def load_memory(state: State, config: RunnableConfig, store: BaseStore):\n",
        "    \"\"\"Loads music preferences from users, if available.\"\"\"\n",
        "\n",
        "    user_id = state[\"customer_id\"] # Get the current customer ID from the state.\n",
        "    namespace = (\"memory_profile\", user_id) # Define a namespace for storing user-specific memory.\n",
        "                                          # This creates a unique key for each user's profile.\n",
        "\n",
        "    # Attempt to retrieve existing memory for this user from the `InMemoryStore`.\n",
        "    existing_memory = store.get(namespace, \"user_memory\")\n",
        "\n",
        "    formatted_memory = \"\" # Initialize formatted memory as empty.\n",
        "\n",
        "    # If memory exists and has a value, format it using our helper function.\n",
        "    if existing_memory and existing_memory.value:\n",
        "        formatted_memory = format_user_memory(existing_memory.value)\n",
        "\n",
        "    # Update the `loaded_memory` field in the state with the retrieved and formatted memory.\n",
        "    return {\"loaded_memory\" : formatted_memory}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf8UJUqUqfDG"
      },
      "outputs": [],
      "source": [
        "# Define a Pydantic BaseModel to structure the `UserProfile` for long-term memory.\n",
        "# This ensures that user preferences are stored in a consistent and verifiable format.\n",
        "class UserProfile(BaseModel):\n",
        "    # `customer_id`: Required field for the customer's unique identifier.\n",
        "    customer_id: str = Field(\n",
        "        description=\"The customer ID of the customer\"\n",
        "    )\n",
        "    # `music_preferences`: A list of strings to store the customer's music interests.\n",
        "    music_preferences: List[str] = Field(\n",
        "        description=\"The music preferences of the customer\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE0lO60zqfDG"
      },
      "outputs": [],
      "source": [
        "# Define the system prompt for the `create_memory` LLM.\n",
        "# This prompt instructs an LLM to act as an analyst, analyzing conversation history\n",
        "# to extract and update user music preferences in a structured `UserProfile` format.\n",
        "create_memory_prompt = \"\"\"You are an expert analyst that is observing a conversation that has taken place between a customer and a customer support assistant. The customer support assistant works for a digital music store, and has utilized a multi-agent team to answer the customer's request.\n",
        "You are tasked with analyzing the conversation that has taken place between the customer and the customer support assistant, and updating the memory profile associated with the customer. The memory profile may be empty. If it's empty, you should create a new memory profile for the customer.\n",
        "\n",
        "You specifically care about saving any music interest the customer has shared about themselves, particularly their music preferences to their memory profile.\n",
        "\n",
        "To help you with this task, I have attached the conversation that has taken place between the customer and the customer support assistant below, as well as the existing memory profile associated with the customer that you should either update or create.\n",
        "\n",
        "The customer's memory profile should have the following fields:\n",
        "- customer_id: the customer ID of the customer\n",
        "- music_preferences: the music preferences of the customer\n",
        "\n",
        "These are the fields you should keep track of and update in the memory profile. If there has been no new information shared by the customer, you should not update the memory profile. It is completely okay if you do not have new information to update the memory profile with. In that case, just leave the values as they are.\n",
        "\n",
        "*IMPORTANT INFORMATION BELOW*\n",
        "\n",
        "The conversation between the customer and the customer support assistant that you should analyze is as follows:\n",
        "{conversation}\n",
        "\n",
        "The existing memory profile associated with the customer that you should either update or create based on the conversation is as follows:\n",
        "{memory_profile}\n",
        "\n",
        "Ensure your response is an object that has the following fields:\n",
        "- customer_id: the customer ID of the customer\n",
        "- music_preferences: the music preferences of the customer\n",
        "\n",
        "For each key in the object, if there is no new information, do not update the value, just keep the value that is already there. If there is new information, update the value.\n",
        "\n",
        "Take a deep breath and think carefully before responding.\n",
        "\"\"\"\n",
        "\n",
        "# Define the `create_memory` node function.\n",
        "# This node is responsible for analyzing the conversation and saving/updating user music preferences.\n",
        "def create_memory(state: State, config: RunnableConfig, store: BaseStore):\n",
        "    user_id = str(state[\"customer_id\"]) # Get the customer ID from the current state (convert to string).\n",
        "    namespace = (\"memory_profile\", user_id) # Define the namespace for this user's memory profile.\n",
        "\n",
        "    # Retrieve the existing memory profile for this user from the long-term store.\n",
        "    existing_memory = store.get(namespace, \"user_memory\")\n",
        "\n",
        "    formatted_memory = \"\" # Initialize formatted memory for the prompt.\n",
        "    if existing_memory and existing_memory.value:\n",
        "        existing_memory_dict = existing_memory.value # Get the dictionary containing the UserProfile instance.\n",
        "        # Format existing music preferences into a string for the prompt.\n",
        "        formatted_memory = (\n",
        "            f\"Music Preferences: {', '.join(existing_memory_dict.get('memory').music_preferences or [])}\" # Access the UserProfile object via 'memory' key\n",
        "        )\n",
        "\n",
        "    # Create a SystemMessage with the formatted prompt, injecting the full conversation history\n",
        "# and the existing memory profile.\n",
        "    formatted_system_message = SystemMessage(content=create_memory_prompt.format(conversation=state[\"messages\"], memory_profile=formatted_memory))\n",
        "\n",
        "    # Invoke the LLM with structured output (`UserProfile`) to analyze the conversation\n",
        "    # and update the memory profile based on new information.\n",
        "    updated_memory = llm.with_structured_output(UserProfile).invoke([formatted_system_message])\n",
        "\n",
        "    key = \"user_memory\" # Define the key for storing this specific memory object.\n",
        "\n",
        "    # Store the updated memory profile back into the `InMemoryStore`.\n",
        "    # We wrap `updated_memory` in a dictionary under the key 'memory' for consistency in access.\n",
        "    store.put(namespace, key, {\"memory\": updated_memory})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6S2P2OCqfDG"
      },
      "outputs": [],
      "source": [
        "# Initialize the final StateGraph for our complete multi-agent system, including memory management.\n",
        "multi_agent_final = StateGraph(State)\n",
        "\n",
        "# Add all necessary nodes to the graph.\n",
        "multi_agent_final.add_node(\"verify_info\", verify_info)         # Node for customer verification\n",
        "multi_agent_final.add_node(\"human_input\", human_input)         # Node for human-in-the-loop interruption\n",
        "multi_agent_final.add_node(\"load_memory\", load_memory)         # Node for loading user long-term memory\n",
        "multi_agent_final.add_node(\"supervisor\", supervisor_prebuilt) # Supervisor for routing to sub-agents\n",
        "multi_agent_final.add_node(\"create_memory\", create_memory)     # Node for saving/updating user long-term memory\n",
        "\n",
        "# Define the initial entry point: all interactions start with verification.\n",
        "multi_agent_final.add_edge(START, \"verify_info\")\n",
        "\n",
        "# Define the conditional routing after `verify_info`.\n",
        "# If verification is successful, proceed to load memory. Otherwise, prompt for human input.\n",
        "multi_agent_final.add_conditional_edges(\n",
        "    \"verify_info\",\n",
        "    should_interrupt,\n",
        "    {\n",
        "        \"continue\": \"load_memory\", # If verified, load user memory\n",
        "        \"interrupt\": \"human_input\", # If not verified, request human input\n",
        "    },\n",
        ")\n",
        "\n",
        "# After `human_input` (resume), loop back to `verify_info` to re-attempt verification.\n",
        "multi_agent_final.add_edge(\"human_input\", \"verify_info\")\n",
        "\n",
        "# After loading memory, proceed to the supervisor for main query processing.\n",
        "multi_agent_final.add_edge(\"load_memory\", \"supervisor\")\n",
        "\n",
        "# After the supervisor completes, save/update the user's memory.\n",
        "multi_agent_final.add_edge(\"supervisor\", \"create_memory\")\n",
        "\n",
        "# The graph ends after memory has been updated.\n",
        "multi_agent_final.add_edge(\"create_memory\", END)\n",
        "\n",
        "# Compile the entire, sophisticated graph.\n",
        "multi_agent_final_graph = multi_agent_final.compile(name=\"multi_agent_verify\", checkpointer=checkpointer, store=in_memory_store)\n",
        "\n",
        "# Display the visualization of the complete graph.\n",
        "show_graph(multi_agent_final_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzjBNXjmqfDG"
      },
      "source": [
        "Let's test it out! We'll use a complex query that requires verification, then touches both music and invoice information, and also includes a music preference that should be saved to long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bBGQotKgqfDG"
      },
      "outputs": [],
      "source": [
        "thread_id = uuid.uuid4() # Generate a fresh unique thread ID for this demonstration.\n",
        "\n",
        "# A comprehensive question that includes customer ID, invoice query, and music preference.\n",
        "question = \"My phone number is +55 (12) 3923-5555. How much was my most recent purchase? What albums do you have by the Rolling Stones?\"\n",
        "\n",
        "# Configuration for the graph invocation.\n",
        "# Note: The user_id is passed as a configurable parameter, although in this specific example,\n",
        "# the customer_id is extracted dynamically by the verify_info node.\n",
        "# For real-world use, ensure consistent handling of user identifiers.\n",
        "config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"1\"}}\n",
        "\n",
        "# Invoke the final multi-agent graph.\n",
        "# This will run through verification, memory loading, supervisor routing (to invoice then music),\n",
        "# and finally memory saving.\n",
        "result = multi_agent_final_graph.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
        "\n",
        "# Print all messages in the final state to observe the complete interaction flow.\n",
        "for message in result[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIcg9Qk9qfDH"
      },
      "source": [
        "Let's take a look at the memory! We expect the music preferences (e.g., 'Rolling Stones') to be saved in our `in_memory_store` under the customer ID associated with the phone number `+55 (12) 3923-5555` (which maps to customer ID 1 in our Chinook DB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "HMHYQ9q7qfDH"
      },
      "outputs": [],
      "source": [
        "user_id = \"1\" # The customer ID we expect to be associated with the phone number used.\n",
        "namespace = (\"memory_profile\", user_id) # The namespace used to store this user's memory.\n",
        "\n",
        "# Retrieve the user's memory profile from the `in_memory_store`.\n",
        "# `.value` retrieves the actual data stored, which should be a dictionary containing the UserProfile instance.\n",
        "memory_data = in_memory_store.get(namespace, \"user_memory\")\n",
        "\n",
        "# Check if memory_data exists and has a 'memory' key (which holds the UserProfile object).\n",
        "if memory_data and \"memory\" in memory_data.value:\n",
        "    saved_music_preferences = memory_data.value.get(\"memory\").music_preferences\n",
        "else:\n",
        "    saved_music_preferences = [] # Default to empty list if no preferences found.\n",
        "\n",
        "print(f\"Saved Music Preferences for Customer ID {user_id}: {saved_music_preferences}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8XfbziBqfDH"
      },
      "source": [
        "# (Optional) Build a Swarm Multi-Agent Graph\n",
        "\n",
        "Beyond the centralized supervisor pattern, another powerful approach to multi-agent systems is the **Swarm architecture**. While the supervisor relies on a single orchestrating agent, a swarm model emphasizes decentralized collaboration.\n",
        "\n",
        "### Swarm Architecture\n",
        "\n",
        "\n",
        "In a swarm, multiple specialized agents work together without a central coordinator. Each agent is aware of the others' capabilities and can *directly hand off* a task to the most appropriate peer when its own expertise is insufficient or when a query crosses domains. This creates a flexible, dynamic flow where agents pass control to each other as needed.\n",
        "\n",
        "### Swarm vs Supervisor\n",
        "\n",
        "Let's recap the key differences:\n",
        "\n",
        "| Feature            | Supervisor Architecture                    | Swarm Architecture                               |\n",
        "| :----------------- | :----------------------------------------- | :----------------------------------------------- |\n",
        "| **Control Flow**   | Centralized; supervisor routes queries.    | Decentralized; agents hand off to each other.    |\n",
        "| **Decision-making**| Supervisor decides which agent to call next.| Each agent decides who to hand off to.            |\n",
        "| **Hierarchy**      | Clear hierarchy: Supervisor -> Sub-agents. | Flat, collaborative structure.                   |\n",
        "| **Modularity**     | Add/remove sub-agents by updating supervisor.| Agents need to know about possible handoffs.    |\n",
        "| **Predictability** | More predictable routing.                  | More dynamic, emergent behavior.                 |\n",
        "| **Resilience**     | Supervisor is a single point of failure.   | More resilient to individual agent failures.     |\n",
        "\n",
        "The choice between a supervisor and a swarm depends on your use case. If you need strict control and a clear workflow, a supervisor is often better. If your problem benefits from dynamic, collaborative problem-solving and greater adaptability, a swarm might be more suitable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_WAwcunqfDH"
      },
      "outputs": [],
      "source": [
        "from langgraph_swarm import create_handoff_tool, create_swarm # Import utilities for creating swarm agents and handoff tools\n",
        "\n",
        "# Create our handoff tools between agents.\n",
        "# These are special tools that, when called by an agent, signal a transfer of control\n",
        "# to another named agent within the swarm.\n",
        "\n",
        "transfer_to_invoice_agent_handoff_tool = create_handoff_tool(\n",
        "    agent_name = \"invoice_information_agent_with_handoff\", # The name of the target agent for this handoff\n",
        "    description = \"Transfer user to the invoice information agent that can help with invoice information\" # Description for LLM\n",
        ")\n",
        "\n",
        "transfer_to_music_catalog_agent_handoff_tool = create_handoff_tool(\n",
        "    agent_name = \"music_catalog_agent_with_handoff\",\n",
        "    description = \"Transfer user to the music catalog agent that can help with music searches and music catalog information\"\n",
        ")\n",
        "\n",
        "# Recreate our agents, but this time, add the handoff tools to their available tools.\n",
        "# This allows each agent to `request` a handoff to the other when appropriate.\n",
        "\n",
        "# First, combine the handoff tools with the existing specific tools for each agent.\n",
        "invoice_tools_with_handoff = [transfer_to_music_catalog_agent_handoff_tool] + invoice_tools\n",
        "music_tools_with_handoff = [transfer_to_invoice_agent_handoff_tool] + music_tools\n",
        "\n",
        "# Recreate the invoice information agent with its original prompt and its new set of tools (including handoff).\n",
        "invoice_information_agent_with_handoff = create_react_agent(\n",
        "    llm,\n",
        "    invoice_tools_with_handoff,\n",
        "    prompt = invoice_subagent_prompt,\n",
        "    name = \"invoice_information_agent_with_handoff\" # Give it a specific name for the swarm\n",
        ")\n",
        "\n",
        "# Recreate the music catalog agent with its original prompt and its new set of tools (including handoff).\n",
        "# Note: The music catalog agent prompt is generated dynamically, as defined earlier.\n",
        "music_catalog_agent_with_handoff = create_react_agent(\n",
        "    llm,\n",
        "    music_tools_with_handoff,\n",
        "    prompt = generate_music_assistant_prompt(),\n",
        "    name = \"music_catalog_agent_with_handoff\" # Give it a specific name for the swarm\n",
        ")\n",
        "\n",
        "\n",
        "# Create the swarm workflow. `create_swarm` handles the orchestration logic\n",
        "# for agents to hand off to each other without a central supervisor.\n",
        "swarm_workflow = create_swarm(\n",
        "    agents = [invoice_information_agent_with_handoff, music_catalog_agent_with_handoff], # The agents participating in the swarm\n",
        "    default_active_agent = \"invoice_information_agent_with_handoff\", # The agent that will be active first by default\n",
        ")\n",
        "\n",
        "# Compile the swarm graph. This makes it runnable and integrates memory.\n",
        "swarm_agents = swarm_workflow.compile(\n",
        "    checkpointer = checkpointer,\n",
        "    store = in_memory_store\n",
        ")\n",
        "\n",
        "# Display the graph of the swarm. Notice it's different from the supervisor graph,\n",
        "# showing connections for potential handoffs rather than a central hub.\n",
        "show_graph(swarm_agents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEdAkitAqfDH"
      },
      "outputs": [],
      "source": [
        "# Create a new thread for this swarm test.\n",
        "thread_id = uuid.uuid4()\n",
        "\n",
        "# Ask a music-related question.\n",
        "question = \"Do you have any albums by the Rolling Stones?\"\n",
        "\n",
        "# Configure the invocation with the thread ID.\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "# Invoke the swarm agents. Even though the default active agent is `invoice_information_agent_with_handoff`,\n",
        "# it should recognize that the query is for music and hand off to `music_catalog_agent_with_handoff`.\n",
        "result = swarm_agents.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
        "\n",
        "# Print the messages to observe the handoff and the final response.\n",
        "for message in result[\"messages\"]:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPzR8OH2qfDH"
      },
      "source": [
        "## Evaluations\n",
        "\n",
        "**Evaluations** are a quantitative and systematic way to measure the performance of your LLM applications, especially agents. They are critical because LLMs don't always behave predictably—even small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured method to:\n",
        "\n",
        "1.  **Identify Failures**: Pinpoint where and why your agent is not performing as expected.\n",
        "2.  **Compare Versions**: Quantitatively compare different versions of your application (e.g., after prompt changes, model updates, or architectural shifts).\n",
        "3.  **Build Reliability**: Iteratively improve your agent's quality and ensure it meets desired performance benchmarks.\n",
        "\n",
        "An evaluation typically comprises three core components:\n",
        "\n",
        "1.  **A Dataset**: A collection of test inputs and their corresponding expected outputs (ground truth). This serves as the benchmark against which your application's performance is measured.\n",
        "2.  **An Application or Target Function**: The specific piece of your LLM application (e.g., an agent, a chain, or a node) that you want to evaluate. This function takes an input and returns an output.\n",
        "3.  **Evaluators**: Metrics or models (often LLMs themselves, known as \"LLM-as-a-judge\") that score your target function's outputs against the dataset's ground truth or specific criteria.\n",
        "\n",
        "There are many ways to evaluate an agent, depending on what aspect of its performance you want to measure. Today, we will cover three common types of agent evaluations:\n",
        "\n",
        "1.  **Final Response Evaluation**: Assess the quality of the agent's ultimate answer to a user query, treating the agent as a black box.\n",
        "2.  **Single Step Evaluation**: Focus on the performance of a specific, critical step within the agent's execution (e.g., whether it correctly selects a tool).\n",
        "3.  **Trajectory Evaluation**: Analyze the entire sequence of steps (the \"trajectory\") an agent takes, assessing whether it follows the expected path of tool calls and internal reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DomImi5aqfDI"
      },
      "source": [
        "### Evaluating The Final Response\n",
        "\n",
        "Evaluating the final response is the most common and often the simplest way to assess an agent's overall effectiveness. It involves treating the agent as a \"black box\" and focusing solely on whether the final output successfully addresses the user's intent, regardless of the intermediate steps taken.\n",
        "\n",
        "*   **Input**: The original user query.\n",
        "*   **Output**: The agent's final, human-facing response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7khTD8VqfDI"
      },
      "source": [
        "#### 1. Create a Dataset for Final Response Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sutoey1cqfDI"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client # Import the LangSmith Client for dataset and experiment management\n",
        "\n",
        "client = Client() # Initialize the LangSmith client. This will connect to your LangSmith account.\n",
        "\n",
        "# Define a list of example inputs and expected outputs for our dataset.\n",
        "# Each dictionary represents a test case with a 'question' (input) and a 'response' (ground truth output).\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"My name is Aaron Mitchell. My number associated with my account is +1 (204) 452-6452. I am trying to find the invoice number for my most recent song purchase. Could you help me with it?\",\n",
        "        \"response\": \"The Invoice ID of your most recent purchase was 342.\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"I'd like a refund.\",\n",
        "        \"response\": \"I need additional information to help you with the refund. Could you please provide your customer identifier so that we can fetch your purchase history?\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who recorded Wish You Were Here again?\",\n",
        "        \"response\": \"Wish You Were Here is an album by Pink Floyd\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What albums do you have by Coldplay?\",\n",
        "        \"response\": \"There are no Coldplay albums available in our catalog at the moment.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "dataset_name = \"LangGraph 101 Multi-Agent: Final Response\" # Define a name for our dataset.\n",
        "\n",
        "# Check if the dataset already exists in LangSmith to avoid recreation.\n",
        "if not client.has_dataset(dataset_name=dataset_name):\n",
        "    # If not, create the dataset.\n",
        "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "    # Populate the dataset with our examples.\n",
        "    # `inputs` are extracted from the 'question' key, `outputs` from the 'response' key.\n",
        "    client.create_examples(\n",
        "        inputs=[{\"question\": ex[\"question\"]} for ex in examples],\n",
        "        outputs=[{\"response\": ex[\"response\"]} for ex in examples],\n",
        "        dataset_id=dataset.id # Associate examples with the created dataset.\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7CF_2ifqfDI"
      },
      "source": [
        "#### 2. Define Application Logic to be Evaluated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY_ICSClqfDI"
      },
      "outputs": [],
      "source": [
        "import uuid # For generating unique thread IDs\n",
        "from langgraph.types import Command # For resuming graph execution after an interrupt\n",
        "\n",
        "graph = multi_agent_final_graph # Reference our complete, final multi-agent graph\n",
        "\n",
        "async def run_graph(inputs: dict):\n",
        "    \"\"\"Run graph and track the final response for evaluation.\"\"\"\n",
        "    # Creating a unique thread ID for each evaluation run to ensure isolation.\n",
        "    thread_id = uuid.uuid4()\n",
        "    # Configuration for the graph invocation. User ID '10' is used here for a specific test scenario.\n",
        "    configuration = {\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"10\"}}\n",
        "\n",
        "    # Invoke the graph with the initial user question.\n",
        "    # This invocation will likely hit the `human_input` node and interrupt if `customer_id` is not present.\n",
        "    result = await graph.ainvoke({\"messages\": [\n",
        "        { \"role\": \"user\", \"content\": inputs['question']}]}, config = configuration)\n",
        "\n",
        "    # After the first invocation, if an interrupt occurred, resume it.\n",
        "    # We explicitly provide a (simulated) customer ID to pass the verification step.\n",
        "    # The `thread_id` in the config must match the initial invocation to resume the correct state.\n",
        "    result = await graph.ainvoke(Command(resume=\"My customer ID is 10\"), config={\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"10\"}})\n",
        "\n",
        "    # Return the content of the last message in the conversation as the final response.\n",
        "    # This is the output that will be evaluated against the dataset's `response`.\n",
        "    return {\"response\": result['messages'][-1].content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPKDErYzqfDI"
      },
      "source": [
        "#### 3. Define the Evaluator for Final Response\n",
        "\n",
        "Evaluators are functions that take the application's output, the original input, and sometimes the reference output, and return a score or feedback. We can use pre-built evaluators or define our own.\n",
        "\n",
        "##### Using a Pre-built Evaluator (OpenEvals)\n",
        "LangSmith integrates with `openevals`, a library providing ready-to-use LLM-as-a-judge evaluators. The `create_llm_as_judge` function sets up an evaluator that uses an LLM to score responses based on a given prompt (e.g., `CORRECTNESS_PROMPT`).\n",
        "\n",
        "##### Defining a Custom Evaluator (LLM-as-a-Judge)\n",
        "For more specific or nuanced evaluation criteria, you can define your own LLM-as-a-judge evaluator. This involves:\n",
        "1.  **Custom Instructions**: A detailed prompt for the LLM that explains its role as a grader and the criteria for scoring.\n",
        "2.  **Structured Output Schema**: A Pydantic `BaseModel` or `TypedDict` to enforce the format of the LLM's grading output (e.g., `is_correct: bool`, `reasoning: str`).\n",
        "3.  **Evaluator Function**: A Python function that calls the structured LLM with the prompt, inputs, and reference outputs, then extracts the relevant score.\n",
        "\n",
        "This approach gives you maximum flexibility over how your agent's responses are judged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVHDwB4fqfDI"
      },
      "outputs": [],
      "source": [
        "from openevals.llm import create_llm_as_judge # Import the utility to create LLM-as-judge evaluators\n",
        "from openevals.prompts import CORRECTNESS_PROMPT # Import a pre-defined prompt for correctness evaluation\n",
        "\n",
        "# Create an LLM-as-judge evaluator for correctness using the pre-built `CORRECTNESS_PROMPT`.\n",
        "# `feedback_key=\"correctness\"` sets the name of the score reported in LangSmith.\n",
        "# `judge=model` specifies which LLM to use for judging.\n",
        "correctness_evaluator = create_llm_as_judge(\n",
        "    prompt=CORRECTNESS_PROMPT,\n",
        "    feedback_key=\"correctness\",\n",
        "    judge=llm,\n",
        ")\n",
        "\n",
        "# Print the content of the pre-defined correctness prompt to understand its instructions.\n",
        "print(CORRECTNESS_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwq6ro4qfDJ"
      },
      "outputs": [],
      "source": [
        "# Custom definition of LLM-as-judge instructions.\n",
        "    # This prompt provides specific guidelines for the LLM acting as a grader, focusing on factual accuracy.\n",
        "grader_instructions = \"\"\"You are a teacher grading a quiz.\n",
        "\n",
        "You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.\n",
        "\n",
        "Here is the grade criteria to follow:\n",
        "(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.\n",
        "(2) Ensure that the student response does not contain any conflicting statements.\n",
        "(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the ground truth response.\n",
        "\n",
        "Correctness:\n",
        "True means that the student's response meets all of the criteria.\n",
        "False means that the student's response does not meet all of the criteria.\n",
        "\n",
        "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\"\"\"\n",
        "\n",
        "# Define the schema for the LLM-as-judge's output using TypedDict.\n",
        "# This ensures the grading output is structured with a reasoning and a boolean correctness score.\n",
        "class Grade(TypedDict):\n",
        "    \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\"\n",
        "    reasoning: Annotated[str, ..., \"Explain your reasoning for whether the actual response is correct or not.\"]\n",
        "    is_correct: Annotated[bool, ..., \"True if the student response is mostly or exactly correct, otherwise False.\"]\n",
        "\n",
        "# Configure the judge LLM to output structured data according to the `Grade` schema.\n",
        "# `method=\"json_schema\"` ensures JSON-based structured output, `strict=True` enforces strict adherence.\n",
        "grader_llm = llm.with_structured_output(Grade, method=\"json_schema\", strict=True)\n",
        "\n",
        "# Define the custom evaluator function `final_answer_correct`.\n",
        "# This function takes inputs, outputs (from our `run_graph`), and reference outputs (from the dataset).\n",
        "async def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "    \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\n",
        "    # Construct the user prompt for the grader LLM, combining the question, ground truth, and student response.\n",
        "    user = f\"\"\"QUESTION: {inputs['question']}\n",
        "    GROUND TRUTH RESPONSE: {reference_outputs['response']}\n",
        "    STUDENT RESPONSE: {outputs['response']}\"\"\"\n",
        "\n",
        "    # Invoke the structured grader LLM with the system instructions and the user prompt.\n",
        "    # Awaiting the async call as LLM invocations are typically async.\n",
        "    grade = await grader_llm.ainvoke([{\"role\": \"system\", \"content\": grader_instructions}, {\"role\": \"user\", \"content\": user}])\n",
        "\n",
        "    # Return the `is_correct` boolean from the grader's output as the evaluation score.\n",
        "    return grade[\"is_correct\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzaHLaavqfDJ"
      },
      "source": [
        "#### 4. Run the Final Response Evaluation\n",
        "\n",
        "Now we're ready to run our evaluation job using the LangSmith client. The `aevaluate` method orchestrates the entire process:\n",
        "\n",
        "1.  It fetches inputs from the specified `data` (our dataset).\n",
        "2.  For each input, it calls our `run_graph` function.\n",
        "3.  It then passes the `run_graph`'s output, along with the original input and dataset's reference output, to each defined `evaluator`.\n",
        "4.  All results are logged and visible in your LangSmith project, providing a comprehensive report of your agent's performance.\n",
        "\n",
        "Key parameters:\n",
        "*   `run_graph`: Our target function to be evaluated.\n",
        "*   `data`: The name of the dataset created in LangSmith.\n",
        "*   `evaluators`: A list of evaluator functions to apply.\n",
        "*   `experiment_prefix`: A prefix for the experiment name in LangSmith, useful for organizing runs.\n",
        "*   `num_repetitions`: How many times to run each example. (For more robust results, typically >1)\n",
        "*   `max_concurrency`: The maximum number of parallel runs (useful for speeding up evaluation).\n",
        "\n",
        "Upon completion, you can navigate to your LangSmith project to view detailed traces and aggregated scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYm2xTZAqfDJ"
      },
      "outputs": [],
      "source": [
        "# Run the evaluation job asynchronously using the LangSmith client.\n",
        "    # This will execute `run_graph` for each example in the dataset and apply the specified evaluators.\n",
        "experiment_results = await client.aevaluate(\n",
        "    run_graph,                        # The asynchronous function that runs our graph and returns its output\n",
        "    data=dataset_name,                # The name of the LangSmith dataset to use for inputs and references\n",
        "    evaluators=[final_answer_correct, correctness_evaluator], # List of evaluator functions to apply\n",
        "    experiment_prefix=\"agent-Llama-e2e\", # A prefix for the experiment name in LangSmith for better organization\n",
        "    num_repetitions=1,                # Number of times to run each example (1 for quick testing)\n",
        "    max_concurrency=5,                # Maximum concurrent runs to optimize evaluation speed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5lWiMEjqfDJ"
      },
      "source": [
        "### Evaluating a Single Step of the Agent\n",
        "\n",
        "While end-to-end evaluation is important, it can be challenging to debug. Sometimes, an agent might fail overall, but you don't know *which* specific decision or action led to the failure. **Single-step evaluation** allows you to test individual components or critical decisions within your agent's workflow in isolation, similar to unit testing in software development.\n",
        "\n",
        "For our multi-agent system, a critical single step is the **supervisor's routing decision**: does it correctly send the query to the music agent or the invoice agent?\n",
        "\n",
        "*   **Input**: The specific input to that single step (e.g., the user message that the supervisor receives).\n",
        "*   **Output**: The direct output of that step (e.g., the name of the agent the supervisor chose to route to)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2kxbrQOqfDJ"
      },
      "source": [
        "#### 1. Create a Dataset for Single-Step Evaluation\n",
        "\n",
        "For single-step evaluation, our dataset's `inputs` will be the user message, and the `outputs` will be the *expected routing decision* (i.e., the name of the sub-agent that should be activated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuA1nsZzqfDJ"
      },
      "outputs": [],
      "source": [
        "# Define examples for single-step evaluation, focusing on the supervisor's routing.\n",
        "    # `messages`: The input to the supervisor (the user's query).\n",
        "    # `route`: The expected output of the supervisor (the name of the sub-agent it should route to).\n",
        "examples = [\n",
        "    {\n",
        "        \"messages\": \"My customer ID is 1. What's my most recent purchase? and What albums does the catalog have by U2?\",\n",
        "        \"route\": 'transfer_to_invoice_information_subagent' # Expects initial routing to invoice agent\n",
        "    },\n",
        "    {\n",
        "        \"messages\": \"What songs do you have by U2?\",\n",
        "        \"route\": 'transfer_to_music_catalog_subagent' # Expects routing to music agent\n",
        "    },\n",
        "    {\n",
        "        \"messages\": \"My name is Aaron Mitchell. My number associated with my account is +1 (204) 452-6452. I am trying to find the invoice number for my most recent song purchase. Could you help me with it?\",\n",
        "        \"route\": 'transfer_to_invoice_information_subagent' # Expects routing to invoice agent\n",
        "    },\n",
        "    {\n",
        "        \"messages\": \"Who recorded Wish You Were Here again? What other albums by them do you have?\",\n",
        "        \"route\": 'transfer_to_music_catalog_subagent' # Expects routing to music agent\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "dataset_name = \"LangGraph 101 Multi-Agent: Single-Step\" # Name for this specific dataset.\n",
        "# Check and create the dataset in LangSmith if it doesn't already exist.\n",
        "if not client.has_dataset(dataset_name=dataset_name):\n",
        "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "    client.create_examples(\n",
        "        inputs = [{\"messages\": ex[\"messages\"]} for ex in examples],\n",
        "        outputs = [{\"route\": ex[\"route\"]} for ex in examples],\n",
        "        dataset_id=dataset.id\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYuTE2wsqfDJ"
      },
      "source": [
        "#### 2. Define the Application Logic to Evaluate (Single Step)\n",
        "\n",
        "To evaluate only the supervisor's routing, we need to run our `supervisor_prebuilt` graph but *interrupt* its execution immediately after the supervisor makes its routing decision, before any sub-agents are actually invoked. LangGraph's `interrupt_before` argument is perfect for this.\n",
        "\n",
        "The `interrupt_before` parameter tells the graph to pause execution right before entering the specified nodes. In this case, we want to pause before `music_catalog_subagent` or `invoice_information_subagent` are invoked. This allows us to inspect the state and determine what the supervisor decided to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX2mjEP8qfDJ"
      },
      "outputs": [],
      "source": [
        "async def run_supervisor_routing(inputs: dict):\n",
        "    \"\"\"Runs the supervisor graph up to the point of routing and returns the chosen route.\"\"\"\n",
        "    # Invoke the `supervisor_prebuilt` graph.\n",
        "    # `interrupt_before` specifies that the graph should pause execution just before entering\n",
        "    # either the music or invoice sub-agent nodes. This captures the routing decision.\n",
        "    # A dummy `user_id` and `thread_id` are provided for configuration, as the supervisor itself doesn't need real verification here.\n",
        "    result = await supervisor_prebuilt.ainvoke(\n",
        "        {\"messages\": [HumanMessage(content=inputs['messages'])]},\n",
        "        interrupt_before=[\"music_catalog_subagent\", \"invoice_information_subagent\"],\n",
        "        config={\"configurable\": {\"thread_id\": uuid.uuid4(), \"user_id\" : \"10\"}}\n",
        "    )\n",
        "\n",
        "    # The name of the last message (which is typically the `tool_call` or `message` that represents the routing decision)\n",
        "    # should correspond to the name of the next chosen sub-agent. This is how the supervisor indicates its routing.\n",
        "    return {\"route\": result[\"messages\"][-1].name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQdg6Kz-qfDK"
      },
      "source": [
        "#### 3. Define the Evaluator for Single Step\n",
        "\n",
        "For this single-step evaluation, a simple exact match evaluator is sufficient. It will check if the `route` output by our `run_supervisor_routing` function exactly matches the `route` defined in our dataset's `reference_outputs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUFiWlThqfDK"
      },
      "outputs": [],
      "source": [
        "def correct(outputs: dict, reference_outputs: dict) -> bool:\n",
        "    \"\"\"Evaluator function to check if the agent chose the correct route.\"\"\"\n",
        "    # Compares the 'route' returned by our `run_supervisor_routing` function\n",
        "    # with the 'route' specified in the ground truth dataset.\n",
        "    return outputs['route'] == reference_outputs[\"route\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1jUhebOqfDK"
      },
      "source": [
        "#### 4. Run the Single Step Evaluation\n",
        "\n",
        "Now we execute the single-step evaluation using `client.aevaluate`, similar to the final response evaluation, but with our specialized function and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-AbfFpJqfDK"
      },
      "outputs": [],
      "source": [
        "experiment_results = await client.aevaluate(\n",
        "    run_supervisor_routing,           # Our function that runs only the supervisor routing step\n",
        "    data=dataset_name,                # The dataset specifically for single-step routing evaluation\n",
        "    evaluators=[correct],\n",
        "    experiment_prefix=\"agent-Llama-singlestep\",\n",
        "    max_concurrency=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG-5BjJbqfDK"
      },
      "source": [
        "### Evaluating the Trajectory of the Agent\n",
        "\n",
        "**Trajectory evaluation** takes a deeper look into the agent's internal workings. Instead of just assessing the final output or a single step, it evaluates the entire sequence of steps (the \"trajectory\") an agent takes to arrive at its answer. This is particularly useful for complex agents where the *process* of reaching a solution is as important as the solution itself (e.g., ensuring a specific set of tools are used in a particular order).\n",
        "\n",
        "*   **Input**: The initial user query to the overall agent.\n",
        "*   **Output**: A detailed list of all nodes/steps visited during the agent's execution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F95cxFTVqfDK"
      },
      "source": [
        "#### 1. Create a Dataset for Trajectory Evaluation\n",
        "\n",
        "For trajectory evaluation, our dataset will contain the user `question` as input and an ordered list of `trajectory` (the expected sequence of node names) as the ground truth output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3UzTLGfqfDK"
      },
      "outputs": [],
      "source": [
        "# Define examples for trajectory evaluation.\n",
        "    # `question`: The user's input.\n",
        "    # `trajectory`: The expected ordered list of node names visited by the graph.\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"My customer ID is 1. What's my most recent purchase? and What albums does the catalog have by U2?\",\n",
        "        \"trajectory\": [\"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Expected path when customer ID is provided\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What songs do you have by U2?\",\n",
        "        \"trajectory\": [\"verify_info\", \"human_input\", \"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Expected path with initial verification and resume\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"My name is Aaron Mitchell. My number associated with my account is +1 (204) 452-6452. I am trying to find the invoice number for my most recent song purchase. Could you help me with it?\",\n",
        "        \"trajectory\": [\"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Expected path when customer ID is provided implicitly\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who recorded Wish You Were Here again? What other albums by them do you have?\",\n",
        "        \"trajectory\": [\"verify_info\", \"human_input\", \"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Another example with initial verification and resume\n",
        "    },\n",
        "]\n",
        "\n",
        "dataset_name = \"LangGraph 101 Multi-Agent: Trajectory Eval\" # Name for this dataset.\n",
        "\n",
        "# Check and create the dataset in LangSmith if it doesn't already exist.\n",
        "if not client.has_dataset(dataset_name=dataset_name):\n",
        "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "    client.create_examples(\n",
        "        inputs=[{\"question\": ex[\"question\"]} for ex in examples],\n",
        "        outputs=[{\"trajectory\": ex[\"trajectory\"]} for ex in examples],\n",
        "        dataset_id=dataset.id\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYeyir8lqfDL"
      },
      "source": [
        "#### 2. Define the Application Logic to Evaluate (Trajectory)\n",
        "\n",
        "To capture the full trajectory, we will use `graph.astream(stream_mode=\"debug\")`. The `debug` stream mode yields detailed `chunk` objects for each step in the graph, including the `task` chunks which contain the name of the node being executed. We'll collect these node names into a list to form the actual trajectory.\n",
        "\n",
        "Similar to the final response evaluation, we need to handle the `human_input` interrupt by resuming the graph with dummy input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6G96pt3qfDL"
      },
      "outputs": [],
      "source": [
        "graph = multi_agent_final_graph # Reference our complete multi-agent graph\n",
        "\n",
        "async def run_graph(inputs: dict) -> dict:\n",
        "    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n",
        "    trajectory = [] # List to store the names of nodes visited\n",
        "    thread_id = uuid.uuid4() # Unique ID for the current thread\n",
        "    # Configuration for the graph invocation, including a dummy user_id for verification step.\n",
        "    configuration = {\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"10\"}}\n",
        "\n",
        "    # First, run the graph for the initial question. `astream` allows us to iterate through chunks.\n",
        "    # `stream_mode=\"debug\"` provides detailed information about each step, including node names.\n",
        "    async for chunk in graph.astream({\"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": inputs['question'],\n",
        "            }\n",
        "        ]}, config = configuration, stream_mode=\"debug\"):\n",
        "        # Check if the chunk type is 'task' (indicating a node execution).\n",
        "        if chunk['type'] == 'task':\n",
        "            # Append the name of the executed node to our trajectory list.\n",
        "            trajectory.append(chunk['payload']['name'])\n",
        "\n",
        "    # If the graph paused for human input, resume it with a dummy customer ID.\n",
        "    async for chunk in graph.astream(Command(resume=\"My customer ID is 10\"), config = configuration, stream_mode=\"debug\"):\n",
        "        if chunk['type'] == 'task':\n",
        "            trajectory.append(chunk['payload']['name'])\n",
        "\n",
        "    # Return the collected trajectory list.\n",
        "    return {\"trajectory\": trajectory}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDRwMBKMqfDL"
      },
      "source": [
        "#### 3. Define the Evaluator for Trajectory\n",
        "\n",
        "For trajectory evaluation, we'll define two custom evaluators:\n",
        "\n",
        "1.  **`evaluate_exact_match`**: This simple evaluator checks if the `actual trajectory` exactly matches the `expected trajectory` from the dataset. It provides a binary score (True/False).\n",
        "2.  **`evaluate_extra_steps`**: This more sophisticated evaluator counts the number of \"unmatched\" or \"extra\" steps taken by the agent that were not present in the reference trajectory. This can indicate inefficiency or unexpected behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITbNMOidqfDL"
      },
      "outputs": [],
      "source": [
        "def evaluate_exact_match(outputs: dict, reference_outputs: dict):\n",
        "    \"\"\"Evaluate whether the trajectory exactly matches the expected output\"\"\"\n",
        "    return {\n",
        "        \"key\": \"exact_match\", # The key for this evaluation metric in LangSmith\n",
        "        \"score\": outputs[\"trajectory\"] == reference_outputs[\"trajectory\"] # True if trajectories are identical\n",
        "    }\n",
        "\n",
        "def evaluate_extra_steps(outputs: dict, reference_outputs: dict) -> dict:\n",
        "    \"\"\"Evaluate the number of unmatched steps in the agent's output trajectory compared to the reference.\"\"\"\n",
        "    i = j = 0 # Pointers for reference trajectory (i) and actual output trajectory (j)\n",
        "    unmatched_steps = 0 # Counter for steps in output not found in reference sequence\n",
        "\n",
        "    # Iterate through both trajectories to find matches and count mismatches.\n",
        "    while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']):\n",
        "        if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:\n",
        "            i += 1  # Match found, move to the next step in reference trajectory\n",
        "        else:\n",
        "            unmatched_steps += 1  # Step in output is not the expected one, count as unmatched\n",
        "        j += 1  # Always move to the next step in outputs trajectory\n",
        "\n",
        "    # After the loop, if there are remaining steps in the output trajectory,\n",
        "    # they are all considered unmatched (extra steps taken by the agent).\n",
        "    unmatched_steps += len(outputs['trajectory']) - j\n",
        "\n",
        "    return {\n",
        "        \"key\": \"unmatched_steps\", # The key for this evaluation metric\n",
        "        \"score\": unmatched_steps, # The count of unmatched steps\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STxAZU5iqfDL"
      },
      "source": [
        "#### 4. Run the Trajectory Evaluation\n",
        "\n",
        "Finally, we run the trajectory evaluation using our specialized `run_graph` function and the two custom trajectory evaluators. The results will be uploaded to LangSmith, where you can analyze the sequence of node executions and compare them against your expected paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLRz3Zz-qfDL"
      },
      "outputs": [],
      "source": [
        "experiment_results = await client.aevaluate(\n",
        "    run_graph,                        # Our function that collects the full trajectory\n",
        "    data=dataset_name,                # The dataset specifically for trajectory evaluation\n",
        "    evaluators=[evaluate_extra_steps, evaluate_exact_match], # Our custom trajectory evaluators\n",
        "    experiment_prefix=\"agent-Llama-trajectory\", # Prefix for the experiment name in LangSmith\n",
        "    num_repetitions=1,\n",
        "    max_concurrency=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGsUyN1NMero"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv-langraph-basic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}